<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>神奇的战士</title>
  
  <subtitle>Problems come up when they come up.</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://wangshub.github.io/"/>
  <updated>2019-04-17T12:31:36.971Z</updated>
  <id>http://wangshub.github.io/</id>
  
  <author>
    <name>神奇的战士</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>当遇到 Scratch3-Codelab，Pando 觉醒了！</title>
    <link href="http://wangshub.github.io/2019/04/17/codelab-pando-tutorial/"/>
    <id>http://wangshub.github.io/2019/04/17/codelab-pando-tutorial/</id>
    <published>2019-04-17T07:17:02.000Z</published>
    <updated>2019-04-17T12:31:36.971Z</updated>
    
    <content type="html"><![CDATA[<h1 id="当遇到-scratch3-codelabpando-觉醒了">当遇到 Scratch3-Codelab，Pando 觉醒了！</h1><blockquote><p>Explore Amazing Moment With Pando</p></blockquote><p><a href="http://www.lejurobot.com/pandocn/" target="_blank" rel="noopener">Pando</a> 是由<a href="http://www.lejurobot.com/" target="_blank" rel="noopener">乐聚机器人</a>研发，可以进行情感互动的益智编程机器人。<strong>主要功能包括</strong>情感交互、手势控制、自主避障、积木编程、动作编程、任务挑战、拟人步态等功能。</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190417164248.png" width="80%"></div><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190417164221.png" width="80%"></div><blockquote><p>图片来源：<a href="http://www.lejurobot.com/" target="_blank" rel="noopener">lejurobot.com</a></p></blockquote><h2 id="scratch3-codelab"><a href="https://codelab-adapter-docs.codelab.club/" target="_blank" rel="noopener">Scratch3-Codelab</a></h2><p>Scratch3-Codelab 是 <a href="https://github.com/wwj718" target="_blank" rel="noopener">wwj718</a> 在 Scratch3 的基础上，集成 EIM (Everything is Message)插件，配合 <a href="https://github.com/Scratch3Lab/codelab_adapter_extensions" target="_blank" rel="noopener">codelab_adapter</a>，就可以让 Scratch3 可以执行 Python 代码，这就赋予了 Scratch3 几乎无限的可能性。</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190417163658.png" width="80%"></div><h2 id="系统框图">系统框图</h2><p>如果可以运行 Python 代码，那么理论上就能把任何智能硬件都接入 Scratch3 当中，而且模块和插件越多，Scratch3 的玩法就越千变万化！</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190417182113.png" width="60%"></div><blockquote><p>图片来源: <a href="https://techziffy.com/self-reconfiguring-modular-robot-market-set-explosive-growth-2026/2234/" target="_blank" rel="noopener">techziffy.com</a></p></blockquote><p>作为一个少儿编程产品，孩子们的想象力其实是远远超过成年的工程师的，如果做到尽可能的开放，让他们去创造和探索机器人的功能，这将是一件非常有意思的事情。</p><p>非常有幸邀请文杰来公司一起交流畅谈，被 CodeLab <strong>万物积木化</strong> 的想法深深吸引，第二天花了一些时间，参照 <a href="https://codelab-adapter-docs.codelab.club/" target="_blank" rel="noopener">Codelab-adapter</a> 文档，为 <a href="http://www.lejurobot.com/pandocn/" target="_blank" rel="noopener">Pando</a> 写了 Codelab 插件 <a href="https://github.com/Scratch3Lab/codelab_adapter_extensions/pull/8" target="_blank" rel="noopener">Pando Robot Extension</a> ，成功接入 Pando 到 Scratch3 当中。下面是系统框图：</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190417172843.png" width="80%"></div><h2 id="视频演示">视频演示</h2><ul><li><strong>YouTube:</strong> <a href="https://www.youtube.com/watch?v=rwj1etkWIl0&amp;feature=youtu.be" target="_blank" rel="noopener">在 Codelab 中控制 Pando 机器人 By 神奇的战士</a></li></ul><iframe width="640" height="480" src="https://www.youtube.com/embed/rwj1etkWIl0" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe><ul><li><strong>Bilibili</strong>: <a href="https://www.bilibili.com/video/av49627582/" target="_blank" rel="noopener">在 Codelab 中控制 Pando 机器人 By 神奇的战士</a></li></ul><iframe width="640" height="480" src="//player.bilibili.com/player.html?aid=49627582&amp;cid=86893839&amp;page=1" scrolling="no" border="0" frameborder="no" framespacing="0" allowfullscreen="true"></iframe><h2 id="软件使用">软件使用</h2><h3 id="macoslinux">MacOS/Linux</h3><ul><li><a href="https://codelab-adapter-docs.codelab.club/user_guide/install/" target="_blank" rel="noopener">安装 codelab-adapter 和使用说明</a></li><li>插件安装<ul><li>Pando 插件 <a href="https://github.com/Scratch3Lab/codelab_adapter_extensions/blob/master/extension_leju_pando.py" target="_blank" rel="noopener">extension_leju_pando.py</a></li><li>蓝牙服务端 <a href="https://github.com/Scratch3Lab/codelab_adapter_extensions/blob/master/servers/pando_server.py" target="_blank" rel="noopener">pando_server.py</a></li></ul></li><li><code>pip3 install pyzmq --user</code></li><li>安装 <a href="https://github.com/adafruit/Adafruit_Python_BluefruitLE" target="_blank" rel="noopener">Adafruit_Python_BluefruitLE</a></li></ul><h3 id="按键控制-pando">按键控制 Pando</h3><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190417161740.png" width="80%"></div><h3 id="微信遥控-pando">微信遥控 Pando</h3><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190417161705.png" width="80%"></div><h3 id="语音控制-pando">语音控制 Pando</h3><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190417161932.png" width="40%"></div><h2 id="更多玩法">更多玩法</h2><p>将 Pando 的运动功能封装成 Block，在 Codelab 平台上就能赋予其更多的 AI 能力，例如</p><ul><li>人脸识别和手势识别控制 Pando</li><li>与 Pando 聊天对话</li><li>Pando 与任何智能硬件交互</li><li>更多有趣的玩法请参考<a href="https://codelab-adapter-docs.codelab.club/user_guide/gallery/" target="_blank" rel="noopener">演示视频</a></li></ul><p><strong>我们相信，孩子的想象力是天马星空的，如果把学习一门编程语言的门槛抹平，他们将能创造出更多令人惊叹的作品。</strong></p><h2 id="参考链接">参考链接</h2><ul><li><a href="https://codelab-adapter-docs.codelab.club/extension_guide/vector/" target="_blank" rel="noopener">Codelab Vector 插件</a></li><li><a href="https://codelab-adapter-docs.codelab.club/" target="_blank" rel="noopener">Welcome to codelab-adapter</a></li><li><a href="https://github.com/adafruit/Adafruit_Python_BluefruitLE" target="_blank" rel="noopener">Adafruit_Python_BluefruitLE</a></li></ul><h2 id="关于作者">关于作者</h2><blockquote><p><a href="https://github.com/wangshub" target="_blank" rel="noopener">神奇的战士</a> 公众号【神奇的战士】<br>来自乐聚的一个工程师 Github: <a href="https://github.com/wangshub" target="_blank" rel="noopener">wangshub</a><br>邮箱：<strong>rocksong.hit@gmail.com</strong></p></blockquote>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;当遇到-scratch3-codelabpando-觉醒了&quot;&gt;当遇到 Scratch3-Codelab，Pando 觉醒了！&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;Explore Amazing Moment With Pando&lt;/p&gt;
&lt;/blockquo
      
    
    </summary>
    
    
      <category term="Python" scheme="http://wangshub.github.io/tags/Python/"/>
    
      <category term="Scratch3" scheme="http://wangshub.github.io/tags/Scratch3/"/>
    
      <category term="Codelab" scheme="http://wangshub.github.io/tags/Codelab/"/>
    
  </entry>
  
  <entry>
    <title>推荐一个 MacOS 上用了就无法自拔的神器 [Hammerspoon] 和我的配置方案</title>
    <link href="http://wangshub.github.io/2019/04/08/hammerspoon-introduce/"/>
    <id>http://wangshub.github.io/2019/04/08/hammerspoon-introduce/</id>
    <published>2019-04-08T14:26:58.000Z</published>
    <updated>2019-04-09T06:12:15.889Z</updated>
    
    <content type="html"><![CDATA[<p>推荐一个 MacOS 上用了就无法自拔的 App</p><p><strong>Hammerspoon</strong>！</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190409090520.png" width="10%"></div><p>Hammerspoon 是 MacOS 上的自动化工具，许多介绍 Hammerspoon 的文章都主要介绍了它是一个窗口管理工具，但是 Hammerspoon 能做到的远远不仅如此。</p><p>Hammerspoon 开源、免费和支持插件，将大多数系统层面的接口封装成了 Lua API，这就让<strong>这把小锤子在 MacOS 上近乎无所不能，可玩性极高</strong>。</p><h2 id="我都用-hammerspoon-做了些什么">我都用 Hammerspoon 做了些什么？</h2><ul><li><strong>菜单栏显示最近几天天气情况</strong><ul><li><a href="https://github.com/wangshub/hammerspoon-config/blob/master/weather/weather.lua" target="_blank" rel="noopener">参考示例</a></li><li>请求免费的天气 API，在深圳这个多雨的城市里提醒我别忘记带伞。</li></ul></li></ul><div style="text-align:center"><img src="https://user-images.githubusercontent.com/20924010/55765634-26490700-5aa3-11e9-9d95-9cf9de6f8f17.gif" width="50%"></div><ul><li><strong>剪切板历史记录</strong><ul><li><a href="https://github.com/wangshub/hammerspoon-config/tree/master/clipboard" target="_blank" rel="noopener">参考示例</a></li><li>记录剪贴板历史，点击某一项再重新复制。<div style="text-align:center"><img src="https://user-images.githubusercontent.com/20924010/55765624-1b8e7200-5aa3-11e9-9f9a-ab271844c8d5.gif" width="53%"></div></li></ul></li><li><strong>音量调节快捷键</strong><ul><li><p><a href="https://github.com/wangshub/hammerspoon-config/tree/master/volume" target="_blank" rel="noopener">参考示例</a></p></li><li><p>当我使用外接键盘时，自定义快捷键 <code>cmd + up/down</code> 调节系统音量</p><div style="text-align:center"><img src="https://user-images.githubusercontent.com/20924010/55765751-c010b400-5aa3-11e9-8b78-e686b104c113.gif" width="54%"></div></li></ul></li><li><strong>窗口管理</strong><ul><li><a href="https://github.com/wangshub/hammerspoon-config/tree/master/window" target="_blank" rel="noopener">参考示例</a></li><li>快捷键实现二分屏、三分屏和全屏</li></ul><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/hammerspoon.2019-04-08%2023_06_30.gif" width="80%"></div></li><li><strong>Wi-Fi 自动脚本</strong><ul><li><a href="https://github.com/wangshub/hammerspoon-config/tree/master/wifi" target="_blank" rel="noopener">参考示例</a></li><li>根据 Wi-Fi SSID 判断是否在公司还是在家，例如在家里自动挂载 NAS 服务器，如果在公司 sshfs 挂载服务器目录等。<div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190408231744.png" width="50%"></div></li></ul></li><li><strong>蓝牙耳机自动连接</strong><ul><li><a href="https://github.com/wangshub/hammerspoon-config/tree/master/headphone" target="_blank" rel="noopener">参考示例</a></li><li>电脑锁屏时，自动断开连接的蓝牙耳机，参考我的<a href="https://zhuanlan.zhihu.com/p/59737941" target="_blank" rel="noopener">另一篇介绍</a>。</li></ul></li><li><strong>输入法自动切换</strong><ul><li><a href="https://github.com/wangshub/hammerspoon-config/tree/master/ime" target="_blank" rel="noopener">参考示例</a></li><li>在每个 App 界面自动切换成搜狗输入法，配合搜狗输入法自动中英文切换，再也不用在系统默认的英文输入法和搜狗输入法之间来回切换了。</li></ul></li><li><strong>定时自动提交代码</strong><ul><li><a href="https://github.com/wangshub/hammerspoon-config/tree/master/autoscript" target="_blank" rel="noopener">参考示例</a></li><li>使用 <code>hs.timer</code> 定时器，定时自动推送我的笔记和下载的电子书到 Github 仓库。</li></ul></li><li><strong>USB 设备连接记录</strong><ul><li><a href="https://github.com/wangshub/hammerspoon-config/tree/master/usb" target="_blank" rel="noopener">参考示例</a></li><li>记录插上你电脑的每一个 USB 设备信息，凡插过必留记录。</li></ul></li><li><strong>消息推送</strong><ul><li><a href="https://github.com/wangshub/hammerspoon-config/tree/master/speaker" target="_blank" rel="noopener">参考示例</a></li><li>推送任意消息提醒</li></ul></li><li><strong>TTS 发声</strong><ul><li><a href="https://github.com/wangshub/hammerspoon-config/tree/master/speaker" target="_blank" rel="noopener">参考示例</a></li><li>调用 <code>say hello world</code> 合成 TTS，模拟真人发音，让 Mac 会说话。</li></ul></li><li><strong>更多... (完全停不下来啊)</strong></li></ul><h2 id="其他插件思路">其他插件思路</h2><ul><li><strong>番茄钟</strong></li><li><strong>应用搜索</strong></li><li><strong>桌面小部件</strong></li><li>...</li></ul><h2 id="使用入门">使用入门</h2><p>Hammerspoon 已经将与 MacOS 之间的系统交互封装成了 <strong>Lua 的 API</strong>，配置 <code>~/.hammerspoon/init.lua</code> 脚本可以与系统进行交互，只需要了解一些基本的 Lua 语法，就可以 <strong>Happy Hacking</strong> 了</p><ul><li><a href="https://learnxinyminutes.com/docs/lua/" target="_blank" rel="noopener">Lua 快速入门教程</a></li><li><a href="https://www.hammerspoon.org/go/" target="_blank" rel="noopener">Hammerspoon API 文档</a></li></ul><h2 id="我的参考配置">我的参考配置</h2><p>如果你懒得配置，可以参考我的配置，根据自身需求修改</p><p><a href="https://github.com/wangshub/hammerspoon-config" target="_blank" rel="noopener">wangshub/hammerspoon-config</a></p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190409090021.png" width="40%"></div><h2 id="最后">最后</h2><p>已经完全停不下来了，Hammerspoon 每天至少省下了 30 分钟的时间 😀⏱</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;推荐一个 MacOS 上用了就无法自拔的 App&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Hammerspoon&lt;/strong&gt;！&lt;/p&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent
      
    
    </summary>
    
    
      <category term="Lua" scheme="http://wangshub.github.io/tags/Lua/"/>
    
      <category term="Hammerspoon" scheme="http://wangshub.github.io/tags/Hammerspoon/"/>
    
  </entry>
  
  <entry>
    <title>升级 Macbook 2015 固态硬盘为  M.2 2280 PCIE NVME 接口 SSD</title>
    <link href="http://wangshub.github.io/2019/04/04/macbook2015upgradessd/"/>
    <id>http://wangshub.github.io/2019/04/04/macbook2015upgradessd/</id>
    <published>2019-04-04T11:18:36.000Z</published>
    <updated>2019-04-04T12:09:55.475Z</updated>
    
    <content type="html"><![CDATA[<h1 id="升级-macbook-2015-固态硬盘为-m.2-2280-pcie-nvme-接口-ssd">升级 Macbook 2015 固态硬盘为 M.2 2280 PCIE NVME 接口 SSD</h1><blockquote><p>东西没有坏掉，只有修不好的</p></blockquote><p>由于苹果原装接口 SSD 价格太高(512G ≈ 2000RMB)，采用 M2.0 接口转苹果 SSD 专用接口的转接卡，升级成本在 600RMB 左右。</p><h2 id="macbook-型号">Macbook 型号</h2><ul><li>MacBook Pro (Retina, 13-inch, Early 2015)</li><li>配置: 8G RAM, 128G SSD</li></ul><h2 id="所需材料">所需材料</h2><ul><li>NVMe PCIe M.2 转 2015 款 Macbook Pro SSD 转接卡 （￥17）</li></ul><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190404192727.png" width="40%"></div><ul><li>INTEL 760p SSD M.2 2280 PCIE NVME 协议固态 （￥656）</li></ul><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190404193211.png" width="30%"></div><ul><li>螺丝刀工具盒</li></ul><h2 id="更换步骤">更换步骤</h2><ul><li><p><a href="https://support.apple.com/zh-cn/HT201372" target="_blank" rel="noopener">制作 MacOS Mojave 启动 U 盘</a></p></li><li><p>自行备份旧 SSD 数据</p></li><li><p>拆解过程请参考 <a href="https://zh.ifixit.com/Guide/MacBook+Pro+%EF%BC%8813%E8%8B%B1%E5%AF%B8%EF%BC%8C%E9%85%8D%E5%A4%87Retina%E6%98%BE%E7%A4%BA%E5%B1%8F%EF%BC%8C2015%E5%88%9D%E6%9C%9F%EF%BC%89%E6%8B%86%E8%A7%A3/38300" target="_blank" rel="noopener">MacBook Pro （13英寸，配备Retina显示屏，2015初期）拆解</a></p></li><li><p>五角螺丝刀拆下 SSD，并更换转接卡</p></li></ul><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190404194132.png" width="50%"></div><ul><li>安装后盖，插上启动 U 盘，开机按照提示安装 MacOS 系统。</li></ul><h2 id="硬盘测速">硬盘测速</h2><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190404194648.png" width="60%"></div><p><strong>OK, 安装完毕，再战 5 年。</strong></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;升级-macbook-2015-固态硬盘为-m.2-2280-pcie-nvme-接口-ssd&quot;&gt;升级 Macbook 2015 固态硬盘为 M.2 2280 PCIE NVME 接口 SSD&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;东西没有坏掉，只有修不好的&lt;
      
    
    </summary>
    
    
      <category term="MacOS" scheme="http://wangshub.github.io/tags/MacOS/"/>
    
      <category term="Macbook" scheme="http://wangshub.github.io/tags/Macbook/"/>
    
  </entry>
  
  <entry>
    <title>『Hamerspoon』蓝牙自动开关</title>
    <link href="http://wangshub.github.io/2019/03/19/Mac-Bluetooth-Auto-Toggle-when-lock-unlock-screen/"/>
    <id>http://wangshub.github.io/2019/03/19/Mac-Bluetooth-Auto-Toggle-when-lock-unlock-screen/</id>
    <published>2019-03-19T06:08:10.000Z</published>
    <updated>2019-04-04T10:10:23.685Z</updated>
    
    <content type="html"><![CDATA[<h1 id="hamerspoonmac-蓝牙自动开关">『Hamerspoon』Mac 蓝牙自动开关</h1><h2 id="前言">前言</h2><p><strong>如果你也像我一样使用 Mac 连接蓝牙耳机，下班后总是忘记关耳机，第二天耳机没电的话，那么这篇文章也许有用。</strong></p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190319165809.png" width="70%"></div><p>当我尝试解决上面这个问题时，写了一个 Python 脚本，但是这并不优雅，直到我找到了 <a href="https://www.hammerspoon.org/" target="_blank" rel="noopener">Hammerspoon</a>，从此打开了新世界大门。</p><h2 id="首先-hammerspoon-是什么">首先 Hammerspoon 是什么？</h2><p>Hammerspoon 是 MacOS 平台上的自动化工具，它<strong>不仅仅是应用窗口管理工具</strong>，配置 <code>~/.hammerspoon/init.lua</code> 脚本可以与系统进行交互，这就让这把小锤子的可玩性极高。</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190319170342.png" width="80%"></div><p>在 <code>init.lua</code> 中，注册一系列的回调函数，当监测到事件发生，就能触发对应的回调，例如</p><ul><li>按下快捷键事件，快速切换应用；</li><li>Wi-Fi 连接事件，判断是否到家了；</li><li>USB 设备连接事件，对插上的设备进行扫描；</li><li>电脑休眠和解锁事件;</li><li>...</li></ul><p><strong>如果使用小锤子，检测到锁屏事件，那么在脚本里就可以自动地对蓝牙进行开关。</strong></p><h2 id="在-mac-上用脚本控制蓝牙开关">在 Mac 上用脚本控制蓝牙开关？</h2><p>在 OSX 上，使用 <a href="https://github.com/toy/blueutil" target="_blank" rel="noopener">blueutil</a> 可以对蓝牙进行操作</p><ul><li>安装方式: <code>brew install blueutil</code></li><li>开启蓝牙: <code>blueutil --power 1</code></li><li>关闭蓝牙: <code>blueutil --power 0</code></li></ul><h2 id="put-them-together">Put Them Together！</h2><ul><li>蓝牙切换函数，这里使用 Hammerspoon 运行 Applescript 脚本，然后再在 Applescript 里运行 shell 指令</li></ul><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">bluetoothSwitch</span><span class="params">(state)</span></span></span><br><span class="line">  <span class="comment">-- state: 0(off), 1(on)</span></span><br><span class="line">  cmd = <span class="string">"/usr/local/bin/blueutil --power "</span>..(state)</span><br><span class="line">  result = hs.osascript.applescript(<span class="built_in">string</span>.<span class="built_in">format</span>(<span class="string">'do shell script "%s"'</span>, cmd))</span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><ul><li>回调函数</li></ul><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">function</span> <span class="title">caffeinateCallback</span><span class="params">(eventType)</span></span></span><br><span class="line">    <span class="keyword">if</span> (eventType == hs.caffeinate.watcher.screensDidSleep) <span class="keyword">then</span></span><br><span class="line">      <span class="built_in">print</span>(<span class="string">"screensDidSleep"</span>)</span><br><span class="line">    <span class="keyword">elseif</span> (eventType == hs.caffeinate.watcher.screensDidWake) <span class="keyword">then</span></span><br><span class="line">      <span class="built_in">print</span>(<span class="string">"screensDidWake"</span>)</span><br><span class="line">    <span class="keyword">elseif</span> (eventType == hs.caffeinate.watcher.screensDidLock) <span class="keyword">then</span></span><br><span class="line">      <span class="built_in">print</span>(<span class="string">"screensDidLock"</span>)</span><br><span class="line">      bluetoothSwitch(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">elseif</span> (eventType == hs.caffeinate.watcher.screensDidUnlock) <span class="keyword">then</span></span><br><span class="line">      <span class="built_in">print</span>(<span class="string">"screensDidUnlock"</span>)</span><br><span class="line">      bluetoothSwitch(<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">end</span></span><br><span class="line"><span class="keyword">end</span></span><br></pre></td></tr></table></figure><ul><li>监测 Mac 系统<code>睡眠</code>、<code>锁屏</code>等事件</li></ul><figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">caffeinateWatcher = hs.caffeinate.watcher.new(caffeinateCallback)</span><br><span class="line">caffeinateWatcher:start()</span><br></pre></td></tr></table></figure><h2 id="安装-hammerspoon-配置">安装 Hammerspoon 配置</h2><ul><li>首先需要安装 <a href="https://github.com/Hammerspoon/hammerspoon/releases/" target="_blank" rel="noopener">Hammerspoon</a></li><li><code>brew install blueutil</code></li><li><code>git clone https://github.com/wangshub/hammerspoon-config.git ~/.hammerspoon</code></li></ul><h2 id="参考链接">参考链接</h2><ul><li><a href="https://www.hammerspoon.org/docs/" target="_blank" rel="noopener">Hammerspoon 文档</a></li><li><a href="https://www.macosxautomation.com/applescript/firsttutorial/index.html" target="_blank" rel="noopener">What is AppleScript?</a></li><li><a href="https://www.lua.org/start.html" target="_blank" rel="noopener">Lua: Getting started</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;hamerspoonmac-蓝牙自动开关&quot;&gt;『Hamerspoon』Mac 蓝牙自动开关&lt;/h1&gt;
&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;如果你也像我一样使用 Mac 连接蓝牙耳机，下班后总是忘记关耳机，第二天耳机没电的话，那么这篇文章也许
      
    
    </summary>
    
    
      <category term="Mac" scheme="http://wangshub.github.io/tags/Mac/"/>
    
      <category term="Lua" scheme="http://wangshub.github.io/tags/Lua/"/>
    
      <category term="Hammerspoon" scheme="http://wangshub.github.io/tags/Hammerspoon/"/>
    
  </entry>
  
  <entry>
    <title>强化学习之 Deep Q­-Learning</title>
    <link href="http://wangshub.github.io/2019/03/11/RLDeepQlearning/"/>
    <id>http://wangshub.github.io/2019/03/11/RLDeepQlearning/</id>
    <published>2019-03-11T12:27:30.000Z</published>
    <updated>2019-04-04T10:10:23.686Z</updated>
    
    <content type="html"><![CDATA[<h1 id="deep-q-learning">Deep Q­-Learning</h1><h2 id="q-learning">Q-Learning</h2><p>在 Q-Learning 中定义函数 <span class="math inline">\(Q(s, a)\)</span> 表示在当前状态 <span class="math inline">\(s\)</span> 下采取动作 <span class="math inline">\(a\)</span> 获得的最大有损奖励</p><p><span class="math display">\[Q\left(s_{t}, a_{t}\right)=\max R_{t+1}\]</span></p><p>可以将 <span class="math inline">\(Q(s, a)\)</span> 理解为在一局游戏中，如果在状态 <span class="math inline">\(s\)</span> 下采取动作 <span class="math inline">\(a\)</span> 后，在游戏结束所能获得的最高分。即 <strong>Q-函数</strong> 表示在某一个状态下采取相应动作的<strong>质量</strong>。</p><p>那么策略 <span class="math inline">\(\pi(s)\)</span> 就可以表示成在每个状态 <span class="math inline">\(s\)</span> 下选择动作 <span class="math inline">\(a\)</span> 的函数</p><p><span class="math display">\[\pi(s)=\operatorname{argmax}_{a} Q(s, a)\]</span></p><p>许多强化学习算法的基本思想都是通过迭代更新 Bellman 方程来对<strong>动作-值</strong>进行估计，最佳的得分奖励是由当前环境的即使奖励 <span class="math inline">\(r\)</span> 和下一个状态 <span class="math inline">\(s^{\prime}\)</span> 的最大奖励的加和。</p><p><span class="math display">\[Q_{i+1}(s, a)=r+\gamma \max _{a^{\prime}} Q_{i}\left(s^{\prime}, a^{\prime}\right)\]</span></p><p>Q-Learning 的核心思想：<strong>用 Bellman 方程迭代近似估计 Q-函数</strong>。一种最简单的方式实现 Q-函数 的方式就是建立一个行为状态，列为动作的表格。</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190312210159.png" width="50%"></div><p>Q-learning 伪代码表示如下</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190312092656.png" width="70%"></div><p>其中 <span class="math inline">\(\alpha\)</span> 成为学习率，用于控制上一时刻的 Q-值 与下一时刻 Q-值 的差值对更新过程的影响，当 <span class="math inline">\(\alpha=1\)</span> 时，上式即为 Bellman 方程。</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190312210239.png" width="80%"></div><p>假设在上图中令机器人从起始位置往右移动 1 步，然后计算并更新 Q-值。</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190313082800.png" width="70%"></div><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190313084707.png" width="70%"></div><p>当 <span class="math inline">\(i \rightarrow \infty\)</span> 时，即可找到<strong>最佳动作值函数</strong> <span class="math inline">\(Q_{i+t} \rightarrow Q^{*}\)</span>。但是在实际当中，每个轨迹序列当中，动作值函数都是被单独估计，不具有任何的泛化的能力。比较常见的方式就是使用(如神经网络)线性或者非线性的估计函数来对动作值函数进行近似估计。</p><h2 id="深度-q-网络deep-q-network">深度 Q 网络(Deep Q Network)</h2><p>假设状态是一张 64*64 分辨率的图像，那么每个像素点用 8bit 的灰度值表示，那么状态 <span class="math inline">\(s\)</span> 就可能有 <span class="math inline">\(256^{64 \times 64}\)</span> 种可能，如果构造这么一张巨大无比的 Q-Table，那将是不现实的。</p><p>对于高度结构化的数据，正好适合用深度神经网络来对 Q 函数进行近似估计。输入状态 <span class="math inline">\(s\)</span> 和动作，通过网络，输出对应的 Q-值。另一种方式是在 DeepMind 文章中，只是输入当前的状态 <span class="math inline">\(s\)</span>, 输出各个 Action 对应的 Q-值。</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190311204446.png" width="80%"></div><p>在 Google DeepMind 的 Paper 中使用了如下的网络结构来对 <span class="math inline">\(\max _{a} \cdot Q\left(s^{\prime}, a^{\prime}\right)\)</span> 进行近似估计</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190311204645.png" width="90%"></div><p>DeepMind 在文章中使用的网络模型如下。</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190313091430.png" width="80%"></div><p>Q 值可能是任意的实数，把对 Q 值的估计看作是机器学习中的<strong>回归问题</strong>，所以使用 <strong>平方误差损失(squared error loss)</strong> 作为优化的目标函数。</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190313195452.png" width="40%"></div><p><span class="math display">\[L_{i}\left(\theta_{i}\right)=\mathbb{E}_{\left(s, a, r, s^{\prime}\right) \sim \mathrm{U}(D)}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)-Q\left(s, a ; \theta_{i}\right)\right)^{2}\right]\]</span></p><p>给定轨迹 <span class="math inline">\(&lt;s, a, r, s^{\prime}&gt;\)</span> ，使用神经网络替换 Q-Table 过程如下</p><ol type="1"><li>输入当前状态 <span class="math inline">\(s\)</span>，预测所有动作 <span class="math inline">\(a\)</span> 的 Q-值；</li><li>输入下一状态 <span class="math inline">\(s\)</span>，计算最大 <span class="math inline">\(\max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)\)</span></li><li>设置 Q 值优化目标为 <span class="math inline">\(r+ \gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime}\right)\)</span></li><li>反向传播更新网络 weights；</li></ol><p>目标 <span class="math inline">\(\max _{a^{\prime}} Q\)</span> 取决于神经网络中神经元的权重。监督学习在学习之前，待估计的目标是确定的。 在优化 <span class="math inline">\(L_{i}\left(\theta_{i}\right)\)</span> 时保持上一次迭代参数 <span class="math inline">\(\theta_{i}^{-}\)</span> 固定不变。</p><p>利用随机梯度下降优化损失函数</p><p><span class="math display">\[\nabla_{\theta_{i}} L\left(\theta_{i}\right)=\mathbb{E}_{s, a, r, s^{\prime}}\left[\left(r+\gamma \max _{a^{\prime}} Q\left(s^{\prime}, a^{\prime} ; \theta_{i}^{-}\right)-Q\left(s, a ; \theta_{i}\right)\right) \nabla_{\theta_{i}} Q\left(s, a ; \theta_{i}\right)\right]\]</span></p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190312083834.png" width="80%"></div><h2 id="经验回放exploration-exploitation">经验回放(Exploration-Exploitation)</h2><p><strong>为什么要经验回放？</strong></p><ul><li>网络是健忘的</li></ul><h2 id="参考文献">参考文献</h2><ul><li><p>https://medium.freecodecamp.org/an-introduction-to-q-learning-reinforcement-learning-14ac0b4493cc</p></li><li></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;deep-q-learning&quot;&gt;Deep Q­-Learning&lt;/h1&gt;
&lt;h2 id=&quot;q-learning&quot;&gt;Q-Learning&lt;/h2&gt;
&lt;p&gt;在 Q-Learning 中定义函数 &lt;span class=&quot;math inline&quot;&gt;\(Q(s, a)
      
    
    </summary>
    
    
      <category term="Python" scheme="http://wangshub.github.io/tags/Python/"/>
    
      <category term="RL" scheme="http://wangshub.github.io/tags/RL/"/>
    
  </entry>
  
  <entry>
    <title>树莓派FM小电台背后的原理解析</title>
    <link href="http://wangshub.github.io/2019/02/26/%E6%A0%91%E8%8E%93%E6%B4%BEFM%E5%B0%8F%E7%94%B5%E5%8F%B0%E8%83%8C%E5%90%8E%E7%9A%84%E5%8E%9F%E7%90%86%E8%A7%A3%E6%9E%90/"/>
    <id>http://wangshub.github.io/2019/02/26/树莓派FM小电台背后的原理解析/</id>
    <published>2019-02-26T12:53:30.000Z</published>
    <updated>2019-04-04T10:10:23.774Z</updated>
    
    <content type="html"><![CDATA[<h1 id="树莓派-fm-发送机小电台背后的原理解析">树莓派 FM 发送机小电台背后的原理解析</h1><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190302161549.png" width="60%"></div><h2 id="介绍">介绍</h2><p>在知乎上有个问题 <a href="https://www.zhihu.com/question/23498424/answer/601685254" target="_blank" rel="noopener">你在 GitHub 上看到过的最有意思的项目是什么？</a> 我介绍了个很有意思的无线 FM 电台的项目，可以在不借用其他外部设备的情况下，通过代码实现将树莓派变为 FM 小电台。</p><p>但是作者只是在<a href="http://www.icrobotics.co.uk/wiki/index.php/Turning" target="_blank" rel="noopener">博客中</a>介绍了大致的原理。于是我搜遍整个网络，发现大多数的相关文章只是教你如何去编译运行代码，对背后的原理却是只言片语，含糊不清。</p><p>作为好奇宝宝，我查阅了许多芯片手册和论坛文章，又翻了翻以前学过的《天线原理》对背后的原理做了一下归纳和总结。</p><h2 id="概念名词解释">概念名词解释</h2><p>首先是一些基本的概念</p><ul><li><strong>FM</strong>: 调频(Frequency Modulation) 是一种以载波的瞬时频率变化来表示信息的调制方式，载波的频率跟随输入信号的幅度直接成等比例变化。FM Radio  就是我们熟悉的调频收音机。</li></ul><div style="text-align:center"><img src="https://i.gifer.com/Gq2S.gif" width="50%"></div><ul><li><strong>PWM</strong>: 脉冲宽度调制(Pulse Width Modulation) 是使用数字源生成模拟信号的方法。主要由 2 个参数来定义：<strong>占空比</strong>和<strong>频率</strong>。如果以保持一定的速率开关数字信号并且保持一定的占空比，那么输出看起来就像恒定电压模拟信号。</li></ul><div style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/1/1f/PWM_3L.gif" width="60%"></div><ul><li><strong>GPIO</strong>: 通用型输入输出(General-purpose input/output)，引脚可以由程序控制作为通用输入(GPI)或者通用输出(GPO)。</li></ul><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190227084604.png" width="70%"></div><ul><li><strong>CPU</strong>: 中央处理器(Central Processing Unit)，相当于树莓派的大脑，功能主要是解释计算机指令以及处理计算机软件中的数据，负责与外围设备通信。树莓派使用的是博通 BCM28XX 系列的 CPU。</li></ul><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190227085239.png" width="50%"></div><ul><li><strong>DMA</strong>: 直接内存访问(Direct Memory Access)这些设备可以执行涉及主内存和其他设备的数据传送。由于设备执行这些操作的过程中无需借助于 CPU，因此该类型的数据传送称为直接内存访问。简单说就是不用  跟 CPU 打招呼就可以直接访问内存。</li></ul><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190227085818.png" width="70%"></div><h2 id="how">HOW?</h2><p>根据<a href="https://zh.wikipedia.org/wiki/%E9%A6%AC%E5%85%8B%E5%A3%AB%E5%A8%81%E6%96%B9%E7%A8%8B%E7%B5%84" target="_blank" rel="noopener"><strong>麦克斯韦的电磁场理论</strong></a></p><blockquote><ol type="1"><li>变化的磁场能够在周围空间产生电场，变化的电场能够在周围空间产生磁场。</li><li>随时间均匀变化的磁场(电场)产生稳定电场(磁场)。随时间不均匀变化的磁场(电场)产生变化的电场(磁场)。</li><li>变化的电场和变化的磁场总是相互关系着，形成一个不可分割的统一体，这就是电磁场。</li></ol></blockquote><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190302181015.png" width="50%"></div><p><strong>随时间变化的电场产生磁场，而随时间变化的磁场又产生电场，两者互为因果。这种不断转化的场统称为电磁场。这种相互的转化形成电磁振荡。</strong></p><p>所以如果在树莓派的 GPIO 上，通过软件控制以一定频率输出高低电平(0/1)，再加上适当长度的天线(一根杜邦线即可)就可以将能量以电磁波的形式发射出去。</p><h3 id="fm-结构图">FM 结构图</h3><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190302170533.png" width="80%"></div><p>由图可知，组成一个 FM 发射机系统，树莓派需要</p><ul><li>信号采样和 FM 调制所需的时钟</li><li>可以通过编程控制电平变化的 GPIO</li><li>一段可以将电磁波发射出去的天线</li></ul><h3 id="时钟">时钟</h3><p>目前绝大多数的的微处理器都有扩频时钟(Spread-spectrum clock)，目的是为了降低<strong>电磁干扰(EMI)</strong>，在树莓派 BCM28XX 系列芯片上，扩频时钟的范围为 1MHz 到 250MHz，这正好用作 FM 的载波信号。</p><p>为了减少 CPU 占用，作者对程序进行了改进，使用树莓派 DMA 产生基础时钟。</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190227212628.png" width="50%"></div><h3 id="信号采样">信号采样</h3><p>由于 FM 广播发送的是音频信号，所以先使用 228 kHz 的采样频率对信号进行采样(满足奈奎斯特采样定理)，带宽为 15 kHz。</p><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190302173206.png" width="80%"></div><h3 id="调频">调频</h3><p>基带信号 <span class="math inline">\(x_{m}(t)\)</span>，载波频率 <span class="math inline">\(f_{c}\)</span>，正弦载波为<span class="math inline">\(x_{c}(t)=A_{c} \cos \left(2 \pi f_{c} t\right)\)</span> 将基带数据信号与载波结合起来得到了传输信号</p><p><span class="math display">\[\begin{aligned} y(t) &amp;=A_{c} \cos \left(2 \pi \int_{0}^{t} f(\tau) d \tau\right) \\ &amp;=A_{c} \cos \left(2 \pi \int_{0}^{t}\left[f_{c}+f_{\Delta} x_{m}(\tau)\right] d \tau\right) \\ &amp;=A_{c} \cos \left(2 \pi f_{c} t+2 \pi f_{\Delta} \int_{0}^{t} x_{m}(\tau) d \tau\right) \end{aligned}\]</span></p><p>其中 <span class="math inline">\(f(\tau)\)</span> 为传输信号的瞬时频率，<span class="math inline">\(f_{\Delta}\)</span> 为频偏表示相对载波频率 <span class="math inline">\(f_{c}\)</span> 的最大频率偏移。</p><p>调频输出的是模拟信号，利用时钟产生 PWM 调整占空比和频率，就可以利用数字信号生成模拟信号。</p><h3 id="树莓派天线长度">树莓派天线长度</h3><ul><li><strong>波长</strong>： <span class="math inline">\(\lambda=c/f\)</span></li><li><strong>偶极子天线</strong>：制作偶极子天线时，会通过工作波长来确定天线的长度。最常见的偶极子天线是半波天线，它的总长度近似为工作波长的一半，即 <span class="math inline">\(L=\lambda / 2\)</span></li></ul><div style="text-align:center"><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a6/Dipole_xmting_antenna_animation_4_408x318x150ms.gif/250px-Dipole_xmting_antenna_animation_4_408x318x150ms.gif" width="50%"></div><p>如果需要发射 100MHz 的 FM 信号，根据上面的公式来计算，就需要 1.5m 长的天线。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="number">3</span>*<span class="number">10</span>**<span class="number">8</span> / (<span class="number">2</span> * <span class="number">100</span> * <span class="number">10</span>**<span class="number">6</span>)</span><br><span class="line"><span class="number">1.5</span></span><br></pre></td></tr></table></figure><p>所以理论上如果给树莓派 GPIO(PIN4) 加上了一根 1.5M 的天线，那么就可以输出最大功率的 FM 信号。</p><p><strong>不要这么做，会干扰正常频段！</strong></p><h4 id="传播距离估计">传播距离估计</h4><p>首先需要计算有效全向辐射功率(EIRP)</p><p><span class="math display">\[EIRP = P - Loss +G\]</span></p><p>其中 <span class="math inline">\(P\)</span> 为发射机的输出功率(<span class="math inline">\(dBm\)</span>)，<span class="math inline">\(Loss\)</span> 为发射机输出端与天线馈源之间的馈线损耗(<span class="math inline">\(dB\)</span>)，<span class="math inline">\(G\)</span> 为天线的发送增益(<span class="math inline">\(dBi\)</span>)。求出 EIRP 后可以进而获得自由空间路径损失（Free Space Path Loss，FSPL）。</p><p>但是使用这个公式估算，意义不是太大。实际测量，如果使用一根 10cm 的杜邦线作为天线，一个楼梯拐角信号就已经非常弱了。</p><h2 id="总结">总结</h2><ul><li>深深佩服第一个作者的 Geek 范和莫大的脑洞;</li><li>不要干扰正常频段，属于违法行为！</li></ul><h2 id="参考">参考</h2><blockquote><p>代码请见参考链接</p></blockquote><ul><li>[1] Eben Upton and Gareth Halfacree. Raspberry Pi user guide. John Wiley &amp; Sons, 2014.</li><li>[2] Oliver Mattos and Oskar Weigl. Turning the Raspberry Pi Into an FM Transmitter. http://www.icrobotics.co.uk/wiki/index.php/Turning the Raspberry Pi Into an FM Transmitter, 2015.</li><li>[3] Christophe Jacquet. FM-RDS transmitter using the Raspberry Pi’s PWM . https://github.com/ChristopheJacquet/PiFmRds, 2014.</li><li>[4] Richardson. Turning the Raspberry Pi Into an FM Transmitter. http://www.icrobotics.co.uk/wiki/index.php/Turning_the_Raspberry_Pi_Into_an_FM_Transmitter, 2015.</li></ul><blockquote><p>欢迎关注我的个人公众号</p></blockquote><div style="text-align:center"><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img/20190302174637.png" width="20%"></div>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;树莓派-fm-发送机小电台背后的原理解析&quot;&gt;树莓派 FM 发送机小电台背后的原理解析&lt;/h1&gt;
&lt;div style=&quot;text-align:center&quot;&gt;
&lt;img src=&quot;https://raw.githubusercontent.com/wangshub
      
    
    </summary>
    
    
      <category term="RaspberryPi" scheme="http://wangshub.github.io/tags/RaspberryPi/"/>
    
  </entry>
  
  <entry>
    <title>用Python给抖音小姐姐打声招呼</title>
    <link href="http://wangshub.github.io/2019/02/20/%E7%94%A8Python%E7%BB%99%E6%8A%96%E9%9F%B3%E5%B0%8F%E5%A7%90%E5%A7%90%E6%89%93%E5%A3%B0%E6%8B%9B%E5%91%BC/"/>
    <id>http://wangshub.github.io/2019/02/20/用Python给抖音小姐姐打声招呼/</id>
    <published>2019-02-20T09:04:28.000Z</published>
    <updated>2019-04-04T10:10:23.775Z</updated>
    
    <content type="html"><![CDATA[<h1 id="用-python-自动给抖音小姐姐评论一个-666">用 Python 自动给抖音小姐姐评论一个 666</h1><p>前一段时间写了一篇 <a href="https://zhuanlan.zhihu.com/p/37365182" target="_blank" rel="noopener">《抖音 Python 机器人，论如何在抖音上找到漂亮小姐姐？》</a>，没想到有相当多的同学有着相同的需求！</p><p>大家皆为</p><p><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img20190220191539.png"></p><p>程序的基本思路请见 <a href="https://github.com/wangshub/Douyin-Bot#%E5%8E%9F%E7%90%86" target="_blank" rel="noopener">原理与使用教程</a></p><p>当程序在抖音上刷到一个漂亮小姐姐的时候就会自动点赞加关注</p><p><img src="https://github.com/wangshub/Douyin-Bot/raw/master/screenshot/auto_reply.gif" width="280/"></p><p>作为一个闷骚程序员，见到漂亮小姐姐总是在<strong>背后默默点赞加关注</strong>。</p><p>但是鲁迅曾经说过 <code>喜欢就要大声说出来！</code></p><p>于是我又给脚本加上了这么一个<strong>自动评论</strong>的功能</p><p><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img20190220191645.png" width="280/"></p><h2 id="自动评论实现步骤">自动评论实现步骤</h2><p>查阅 ADB 文档， <code>adb shell input text "hello,world"</code> 命令可直接将字符串输入到 Android App 的输入框中，但是仅支持 ASICC 字符，也就是不能够输入中文字符。好在 <a href="https://github.com/senzhk/ADBKeyBoard" target="_blank" rel="noopener">senzhk</a> 写了一个 ADB 虚拟键盘输入法，通过监听 ADB 广播事件接收 ADB 发送的 Unicode 编码字符串，然后输入到文本框中，可以绕过这个限制。</p><p><strong>操作步骤</strong></p><ul><li><p>安卓手机安装 <a href="https://github.com/wangshub/Douyin-Bot/blob/master/apk/ADBKeyBoard.apk" target="_blank" rel="noopener">ADBKeyBoard.apk</a></p></li><li><p>将手机默认输入法切换为 <code>ADB KeyBoard</code></p><p><img src="https://raw.githubusercontent.com/wangshub/image-hosting/master/img20190220195516.png" width="240"></p></li><li><p>手机连上电脑，在输入框输入 <code>adb shell am broadcast -a ADB_INPUT_TEXT --es msg '可爱小姐姐你好呀'</code> 即可输入中文字符；</p></li></ul><h2 id="代码实现">代码实现</h2><p>完整代码请参考 <a href="https://github.com/wangshub/Douyin-Bot" target="_blank" rel="noopener">wangshub/Douyin-Bot</a></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">auto_reply</span><span class="params">()</span>:</span></span><br><span class="line">    msg = <span class="string">"垆边人似月，皓腕凝霜雪。就在刚刚，我的心动了一下，小姐姐你好可爱呀_Powered_By_Python"</span></span><br><span class="line">    tap(config[<span class="string">'comment_bottom'</span>][<span class="string">'x'</span>], config[<span class="string">'comment_bottom'</span>][<span class="string">'y'</span>])</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    tap(config[<span class="string">'comment_text'</span>][<span class="string">'x'</span>], config[<span class="string">'comment_text'</span>][<span class="string">'y'</span>])</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    cmd = <span class="string">'shell am broadcast -a ADB_INPUT_TEXT --es msg &#123;text&#125;'</span>.format(text=msg)</span><br><span class="line">    adb.run(cmd)</span><br><span class="line">    time.sleep(<span class="number">1</span>)</span><br><span class="line">    tap(config[<span class="string">'comment_send'</span>][<span class="string">'x'</span>], config[<span class="string">'comment_send'</span>][<span class="string">'y'</span>])</span><br><span class="line">    time.sleep(<span class="number">0.5</span>)</span><br><span class="line">    cmd = <span class="string">'shell input keyevent 4'</span></span><br><span class="line">    adb.run(cmd)</span><br></pre></td></tr></table></figure><h2 id="存在的问题">存在的问题</h2><ul><li><input type="checkbox" disabled>ADB 不支持 Unicode 编码，所以需要用 ADB KeyBoard 操作比较繁琐；</li><li><input type="checkbox" disabled>再者 ADB 是对屏幕坐标点击，存在屏幕分辨率适配的硬伤；</li><li><input type="checkbox" disabled>针对上面的两个问题，可以使用 App 自动化测试工具 <a href="http://appium.io/" target="_blank" rel="noopener">Appium</a> 解决，后面再找机会把坑填上；</li></ul><h2 id="总结">总结</h2><p>小孩子不要玩抖音 ！小孩子不要玩抖音！ 太耗时间了，每天在安装了又卸载，卸载了又安装的 Loop 中循环 O_O</p><blockquote><p>送人一赞，手留余香 🌹 作者：神奇的战士 <a href="https://github.com/wangshub" target="_blank" rel="noopener">https://github.com/wangshub</a> 欢迎关注我的公众号【神奇的战士】，期待跟你分享有趣的代码 ~</p></blockquote><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fs09ydtc98j20vd06p759.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;用-python-自动给抖音小姐姐评论一个-666&quot;&gt;用 Python 自动给抖音小姐姐评论一个 666&lt;/h1&gt;
&lt;p&gt;前一段时间写了一篇 &lt;a href=&quot;https://zhuanlan.zhihu.com/p/37365182&quot; target=&quot;_blan
      
    
    </summary>
    
    
      <category term="Python" scheme="http://wangshub.github.io/tags/Python/"/>
    
      <category term="ADB" scheme="http://wangshub.github.io/tags/ADB/"/>
    
      <category term="Douyin" scheme="http://wangshub.github.io/tags/Douyin/"/>
    
  </entry>
  
  <entry>
    <title>在GitHub上看到过的最有意思的项目</title>
    <link href="http://wangshub.github.io/2019/02/18/%E5%9C%A8GitHub%E4%B8%8A%E7%9C%8B%E5%88%B0%E8%BF%87%E7%9A%84%E6%9C%80%E6%9C%89%E6%84%8F%E6%80%9D%E7%9A%84%E9%A1%B9%E7%9B%AE/"/>
    <id>http://wangshub.github.io/2019/02/18/在GitHub上看到过的最有意思的项目/</id>
    <published>2019-02-18T07:02:53.000Z</published>
    <updated>2019-04-04T10:10:23.770Z</updated>
    
    <content type="html"><![CDATA[<h1 id="在github上看到过的最有意思的项目">在GitHub上看到过的最有意思的项目</h1><h3 id="键盘听声识键">键盘听声识键</h3><ul><li>地址：<a href="https://github.com/ggerganov/kbd-audio" target="_blank" rel="noopener">ggerganov/kbd-audio</a></li><li>通过麦克风录取键盘的敲击声，判断你按下的是哪个按键。kbd-audio 提供了一整套 GUI 工具，帮助你可视化分析输入的语音数据。从此在输入密码的时候得小心了 -_-</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://i.imgur.com/mnRvT1X.gif" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="通过-wi-fi-信号估计附近有多少人">通过 Wi-Fi 信号估计附近有多少人</h3><ul><li>地址：<a href="https://github.com/schollz/howmanypeoplearearound" target="_blank" rel="noopener">schollz/howmanypeoplearearound</a></li><li>用一个支持<a href="https://github.com/wangshub/hmpa-pi#%E7%9B%91%E5%90%AC%E6%A8%A1%E5%BC%8F%E7%9A%84%E7%BD%91%E5%8D%A1" target="_blank" rel="noopener">监听模式</a>的 USB 网卡，就可以扫描出附近 Wi-Fi 的网络内的设备与信号强度。按照不同国家地区人群拥有手机比例，就可以大致估计附近人数。</li></ul><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">$ howmanypeoplearearound -o test.json -a wlan1</span><br><span class="line">[==================================================] 100%         0s left</span><br><span class="line">There are about 4 people around.</span><br><span class="line">$ cat test.json | python3 -m json.tool</span><br><span class="line">[</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;rssi&quot;: -86.0,</span><br><span class="line">    &quot;mac&quot;: &quot;90:e7:c4:xx:xx:xx&quot;,</span><br><span class="line">    &quot;company&quot;: &quot;HTC Corporation&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;rssi&quot;: -84.0,</span><br><span class="line">    &quot;mac&quot;: &quot;80:e6:50:xx:xx:xx&quot;,</span><br><span class="line">    &quot;company&quot;: &quot;Apple, Inc.&quot;</span><br><span class="line">  &#125;,</span><br><span class="line">  &#123;</span><br><span class="line">    &quot;rssi&quot;: -49.0,</span><br><span class="line">    &quot;mac&quot;: &quot;ac:37:43:xx:xx:xx&quot;,</span><br><span class="line">    &quot;company&quot;: &quot;HTC Corporation&quot;</span><br><span class="line">  &#125;</span><br><span class="line">]</span><br></pre></td></tr></table></figure><h3 id="树莓派-fm-发送站">树莓派 FM 发送站</h3><ul><li>地址：<a href="https://github.com/Miegl/PiFmAdv" target="_blank" rel="noopener">Miegl/PiFmAdv</a></li><li>对音频信号进行 FM 调制，树莓派的 GPIO 口作为天线发射信号，一个小小的私人电台就搭建好了。</li><li>注意：私人架设电台是违法行为，不能发射大功率的信号干扰正常的FM频段!</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://github.com/miegl/PiFmAdv/raw/master/doc/galaxy_s2.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="github-仓库代码隐私数据扫描">Github 仓库代码隐私数据扫描</h3><ul><li>地址: <a href="https://github.com/dxa4481/truffleHog" target="_blank" rel="noopener">dxa4481/truffleHog</a></li><li>有相当多的人会在自己的 Github 开源项目中遗留一些账号密码，truffleHog 可以在对 Git 的所有分支和 Commit 记录中搜索密钥等敏感信息。</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://camo.githubusercontent.com/3972f94b479bb40fb690e2cadcd3dd14ab3c3073/68747470733a2f2f692e696d6775722e636f6d2f5941586e644c442e706e67" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="wi-fi-破解">Wi-Fi 破解</h3><ul><li>地址：<a href="https://github.com/brannondorsey/wifi-cracking" target="_blank" rel="noopener">brannondorsey/wifi-cracking</a></li><li>该项目详细记录了用 Airodump-ng 和 Aircrack-ng/Hashcat 破解 WPA/WPA2 无线 Wi-Fi 路由器的过程和思路。</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="http://ultimatepeter.com/wp-content/uploads/2013/09/hack-wi-fi-cracking-wpa2-psk-passwords-with-cowpatty.w654.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><p><strong>最后还有这个 ...</strong></p><h3 id="v9porn">v9porn</h3><ul><li>地址：<a href="https://github.com/techGay/v9porn" target="_blank" rel="noopener">techGay/v9porn</a></li><li>这个项目就不多介绍了，自己体会。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;在github上看到过的最有意思的项目&quot;&gt;在GitHub上看到过的最有意思的项目&lt;/h1&gt;
&lt;h3 id=&quot;键盘听声识键&quot;&gt;键盘听声识键&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;地址：&lt;a href=&quot;https://github.com/ggerganov/kbd-audi
      
    
    </summary>
    
    
      <category term="Code" scheme="http://wangshub.github.io/tags/Code/"/>
    
      <category term="Github" scheme="http://wangshub.github.io/tags/Github/"/>
    
  </entry>
  
  <entry>
    <title>Vanilla_Policy_Gradient</title>
    <link href="http://wangshub.github.io/2019/01/29/Vanilla-Policy-Gradient/"/>
    <id>http://wangshub.github.io/2019/01/29/Vanilla-Policy-Gradient/</id>
    <published>2019-01-29T08:48:53.000Z</published>
    <updated>2019-04-04T10:10:23.687Z</updated>
    
    <content type="html"><![CDATA[<h1 id="强化学习-vanilla-policy-gradient">强化学习 Vanilla Policy Gradient</h1><p>令 <span class="math inline">\(\pi _ { \theta }\)</span> 表示参数为 <span class="math inline">\(\theta\)</span> 的策略，<span class="math inline">\(J \left( \pi _ { \theta } \right)\)</span> 表示策略 <span class="math inline">\(\pi _ { \theta }\)</span> 的返回值，则优化函数梯度为</p><p><span class="math display">\[\nabla _ { \theta } J \left( \pi _ { \theta } \right) = \underset { \tau \sim \pi _ { \theta } } { E } \left[ \sum _ { t = 0 } ^ { T } \nabla _ { \theta } \log \pi _ { \theta } \left( a _ { t } | s _ { t } \right) A ^ { \pi _ { \theta } } \left( s _ { t } , a _ { t } \right) \right]\]</span></p><p>梯度上升</p><p><span class="math display">\[\theta _ { k + 1 } = \theta _ { k } + \alpha \nabla _ { \theta } J \left( \pi _ { \theta _ { k } } \right)\]</span></p><p>算法伪代码</p><p><img src="https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190129183255.png"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;强化学习-vanilla-policy-gradient&quot;&gt;强化学习 Vanilla Policy Gradient&lt;/h1&gt;
&lt;p&gt;令 &lt;span class=&quot;math inline&quot;&gt;\(\pi _ { \theta }\)&lt;/span&gt; 表示参数为 &lt;s
      
    
    </summary>
    
    
      <category term="Reinforcement learning" scheme="http://wangshub.github.io/tags/Reinforcement-learning/"/>
    
      <category term="Algorithm" scheme="http://wangshub.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>用Python获取B站播放历史记录</title>
    <link href="http://wangshub.github.io/2019/01/22/%E7%94%A8Python%E8%8E%B7%E5%8F%96B%E7%AB%99%E6%92%AD%E6%94%BE%E5%8E%86%E5%8F%B2%E8%AE%B0%E5%BD%95/"/>
    <id>http://wangshub.github.io/2019/01/22/用Python获取B站播放历史记录/</id>
    <published>2019-01-22T10:40:22.000Z</published>
    <updated>2019-04-04T10:10:23.775Z</updated>
    
    <content type="html"><![CDATA[<h1 id="用-python-获取-b-站播放历史记录">用 Python 获取 B 站播放历史记录</h1><p>最近 B 站出了一个年度报告，统计用户一年当中在 B 站上观看视频的总时长和总个数。过去一年我居然在 B 站上看了 <code>2600+</code> 个视频，总计 <code>251</code> 个小时，居然花了这么多时间，吓得我差点把 Bilibili App 卸载了...</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fzfksqinlgj208i07475z.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><p>然而我又很好奇，到底我在 B 站上都看了些什么类型 <del>小姐姐</del> 的视频，用几行 Python 代码实现了一下。</p><h2 id="获取请求-api-接口与-cookie">获取请求 Api 接口与 Cookie</h2><p>实现起来非常容易，获取 cookie 模拟请求即可</p><ol type="1"><li><p>使用 chrome 浏览器</p></li><li><p>登陆 <a href="https://www.bilibili.com" target="_blank" rel="noopener">B 站</a>，进入历史记录 <a href="https://www.bilibili.com/account/history" target="_blank" rel="noopener">https://www.bilibili.com/account/history</a></p></li><li><p>在网页任意位置，鼠标右键 <code>检查</code></p></li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fzfkf5qyuqj20c009g765.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><ol start="4" type="1"><li>按照下图所示，进入 <code>Network</code> 页面，筛选框输入 <code>history</code>，对结果进行筛选，页面滚轮往下即可看到浏览过程中的历史记录请求的 <code>Header</code></li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fzfkc5s8scj21ga0nok4i.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><ol start="5" type="1"><li>将 Header 下， cookie 一行的字符串复制出来到一个 <code>cookie.txt</code> 文本里</li></ol><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fzfkkj1adsj20ta07ita2.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><h2 id="python-代码实现">Python 代码实现</h2><ul><li>伪造浏览器请求</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> json</span><br><span class="line"><span class="keyword">import</span> requests</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">read_cookies_file</span><span class="params">(filename)</span>:</span></span><br><span class="line">    <span class="string">"""read cookie txt file</span></span><br><span class="line"><span class="string">    :param filename: (str) cookies file path</span></span><br><span class="line"><span class="string">    :return: (dict) cookies</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">with</span> open(filename, <span class="string">'r'</span>) <span class="keyword">as</span> fp:</span><br><span class="line">        cookies = fp.read()</span><br><span class="line">        <span class="keyword">return</span> cookies</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_header</span><span class="params">(filename)</span>:</span></span><br><span class="line">    cookie = read_cookies_file(filename)</span><br><span class="line">    headers = &#123;</span><br><span class="line">        <span class="string">'Accept'</span>: <span class="string">'*/*'</span>,</span><br><span class="line">        <span class="string">'Accept-Encoding'</span>: <span class="string">'gzip, deflate, br'</span>,</span><br><span class="line">        <span class="string">'Accept-Language'</span>: <span class="string">'zh-CN,zh;q=0.9,en-US;q=0.8,en;q=0.7'</span>,</span><br><span class="line">        <span class="string">'Connection'</span>: <span class="string">'keep-alive'</span>,</span><br><span class="line">        <span class="string">'Cookie'</span>: cookie,</span><br><span class="line">        <span class="string">'Host'</span>: <span class="string">'api.bilibili.com'</span>,</span><br><span class="line">        <span class="string">'Referer'</span>: <span class="string">'https://www.bilibili.com/account/history'</span>,</span><br><span class="line">        <span class="string">'User-Agent'</span>: <span class="string">'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14_2) AppleWebKit/537.36 '</span></span><br><span class="line">                      <span class="string">'(KHTML, like Gecko) Chrome/71.0.3578.98 Safari/537.36'</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> headers</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">req_get</span><span class="params">(headers, url)</span>:</span></span><br><span class="line">    resp = requests.get(url, headers=headers)</span><br><span class="line">    <span class="keyword">return</span> json.loads(resp.text)</span><br></pre></td></tr></table></figure><ul><li>使用 cookie 模拟请求</li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_all_bili_history</span><span class="params">(cookie_file)</span>:</span></span><br><span class="line">    headers = bilibili.get_header(cookie_file)</span><br><span class="line">    history = &#123;<span class="string">'all'</span>: []&#125;</span><br><span class="line">    <span class="keyword">for</span> page_num <span class="keyword">in</span> range(MAX_PAGE):</span><br><span class="line">        time.sleep(<span class="number">0.6</span>)</span><br><span class="line">        url = <span class="string">'https://api.bilibili.com/x/v2/history?pn=&#123;pn&#125;&amp;ps=&#123;ps&#125;&amp;jsonp=jsonp'</span>.format(pn=page_num, ps=PAGE_PER_NUM)</span><br><span class="line">        result = bilibili.req_get(headers, url)</span><br><span class="line">        print(<span class="string">'page = &#123;&#125; code = &#123;&#125; datalen = &#123;&#125;'</span>.format(page_num, result[<span class="string">'code'</span>], len(result[<span class="string">'data'</span>])))</span><br><span class="line">        <span class="keyword">if</span> len(result[<span class="string">'data'</span>]) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        history[<span class="string">'all'</span>].append(result)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> history</span><br></pre></td></tr></table></figure><ul><li>代码非常简单，完整代码在 <a href="https://github.com/wangshub/bilibili-history" target="_blank" rel="noopener">https://github.com/wangshub/bilibili-history</a></li></ul><h2 id="存在的问题">存在的问题</h2><ul><li><p>本来想拿到所有的播放记录，做一些统计和预测，但是经过实测，B 站只能获取到最近 <code>1000</code> 条或者最近 <code>3</code> 个月的播放记录</p></li><li><p>如果想获得更多，只能做一个监测程序，不停地从接口获取数据</p></li></ul><h2 id="安全问题">安全问题</h2><p>尽量不要使用不安全的 wifi 网络，有可能会被别有用心之人获取网络请求的 Package，易泄露个人隐私。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;用-python-获取-b-站播放历史记录&quot;&gt;用 Python 获取 B 站播放历史记录&lt;/h1&gt;
&lt;p&gt;最近 B 站出了一个年度报告，统计用户一年当中在 B 站上观看视频的总时长和总个数。过去一年我居然在 B 站上看了 &lt;code&gt;2600+&lt;/code&gt; 个视
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>强化学习:策略梯度算法</title>
    <link href="http://wangshub.github.io/2019/01/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0:%E7%AD%96%E7%95%A5%E6%A2%AF%E5%BA%A6%E7%AE%97%E6%B3%95/"/>
    <id>http://wangshub.github.io/2019/01/15/强化学习:策略梯度算法/</id>
    <published>2019-01-15T11:46:17.000Z</published>
    <updated>2019-04-04T10:10:23.772Z</updated>
    
    <content type="html"><![CDATA[<h1 id="强化学习策略梯度算法">强化学习:策略梯度算法</h1><h2 id="策略梯度的公式推导">策略梯度的公式推导</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190115200214.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>​ 学习<strong>参数化表示的策略</strong> (Parameterized policy), 输入环境状态 $ S $ 来选择动作 <span class="math inline">\(a\)</span> ，这里使用 <span class="math inline">\(\theta \in \mathbb { R } ^ { d }\)</span> 来表示策略的参数向量，因此策略函数表示为</p><p><span class="math display">\[\pi ( a | s , \boldsymbol { \theta } ) = \operatorname { P_r } \left\{ A _ { t } = a | S _ { t } = s , \boldsymbol { \theta } _ { t } = \boldsymbol { \theta } \right\} \tag{1} \]</span></p><p>其中时刻 <span class="math inline">\(t\)</span> ，环境状态为 <span class="math inline">\(s\)</span> ，参数为 <span class="math inline">\(\theta\)</span> ，输出动作 <span class="math inline">\(a\)</span> 的概率为 <span class="math inline">\(P_r\)</span></p><p>因此生成马尔可夫决策过程的一个<em>轨迹</em>（trajectory）<span class="math inline">\(\tau = (\mathbf { s } _ { 1 } , \mathbf { a } _ { 1 } , \dots , \mathbf { s } _ { T } , \mathbf { a } _ { T })\)</span> 的概率为</p><p><span class="math display">\[\underbrace { p _ { \theta } \left( \mathbf { s } _ { 1 } , \mathbf { a } _ { 1 } , \ldots , \mathbf { s } _ { T } , \mathbf { a } _ { T } \right) } _ { \pi _ { \theta } ( \tau ) } = p \left( \mathbf { s } _ { 1 } \right) \prod _ { t = 1 } ^ { T } \pi _ { \theta } \left( \mathbf { a } _ { t } | \mathbf { s } _ { t } \right) p \left( \mathbf { s } _ { t + 1 } | \mathbf { s } _ { t } , \mathbf { a } _ { t } \right) \tag{2} \]</span></p><p>更一般地，将策略 <span class="math inline">\(\pi\)</span> 下生成轨迹 <span class="math inline">\(\tau\)</span> 的概率表示为</p><p><span class="math display">\[P ( \tau | \pi ) = \rho _ { 1 } \left( s _ { 1 } \right) \prod _ { t = 0 } ^ { T  } P \left( s _ { t + 1 } | s _ { t } , a _ { t } \right) \pi \left( a _ { t } | s _ { t } \right) \tag{3}\]</span></p><p>​ 策略梯度方法的目标就是找到一组最佳的参数 <span class="math inline">\(\theta ^ { \star }\)</span> 来表示策略函数使得累计奖励的期望最大，即</p><p><span class="math display">\[\theta ^ { \star } = \arg \max _ { \theta } E _ { \tau \sim p _ { \theta } ( \tau ) } \left[ \sum _ { t } r \left( \mathbf { s } _ { t } , \mathbf { a } _ { t } \right) \right] \tag{4}\]</span></p><p>​ 令累积奖励为 <span class="math inline">\(R ( \tau ) = \sum _ { t = 1 } ^ { T } r \left( s _ { t } , a _ { t } \right)\)</span> ，设定优化目标 <span class="math inline">\(J\left( \pi _ { \theta } \right)\)</span> 优化策略参数使得奖励的期望值最大</p><p><span class="math display">\[J\left( \pi _ { \theta } \right) = \underset { \tau \sim \pi _ { \theta } } { \mathrm { E } } [ R ( \tau ) ]\tag{5} \]</span></p><p>对 <span class="math inline">\(J\left( \pi _ { \theta } \right)\)</span> 求梯度可得策略梯度 $_ { } J ( ) $ ，公式 (6) 的推导过程请参见<a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro3.html#deriving-the-simplest-policy-gradient" target="_blank" rel="noopener">链接</a></p><p><span class="math display">\[\begin{aligned} \nabla _ { \theta } J ( \theta ) &amp;= \int \nabla _ { \theta } \pi _ { \theta } ( \tau ) r ( \tau ) d \tau \\&amp;= \int \pi _ { \theta } ( \tau ) \nabla _ { \theta } \log \pi _ { \theta } ( \tau ) r ( \tau ) d \tau \\ &amp;= E _ { \tau \sim \pi _ { \theta } ( \tau ) } \left[ \nabla _ { \theta } \log \pi _ { \theta } ( \tau ) r ( \tau ) \right]\end{aligned} \tag{6}\]</span></p><p>将策略 (1) 两边取 log 对数，然后带入梯度表达式 (6) ，推导策略梯度的公式请参考下图</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190117080942.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>根据策略 <span class="math inline">\(\pi _ { \theta }\)</span> 生成 <span class="math inline">\(N\)</span> 条轨迹如图所示</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190117084618.png" alt title>                </div>                <div class="image-caption"></div>            </figure><p>利用上图 <span class="math inline">\(N\)</span> 条轨迹的经验平均对策略梯度进行逼近，有公式 (7) (8)</p><p><span class="math display">\[J ( \theta ) = E _ { \tau \sim p _ { \theta } ( \tau ) } \left[ \sum _ { t } r \left( \mathbf { s } _ { t } , \mathbf { a } _ { t } \right) \right] \approx \frac { 1 } { N } \sum _ { i } \sum _ { t } r \left( \mathbf { s } _ { i , t } , \mathbf { a } _ { i , t } \right) \tag{7}\]</span></p><p><span class="math display">\[\nabla _ { \theta } J ( \theta ) \approx \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \left( \sum _ { t = 1 } ^ { T } \nabla _ { \theta } \log \pi _ { \theta } \left( \mathbf { a } _ { i , t } | \mathbf { s } _ { i , t } \right) \right) \left( \sum _ { t = 1 } ^ { T } r \left( \mathbf { s } _ { i , t } , \mathbf { a } _ { i , t } \right) \right) \tag{8}\]</span></p><p>其中 <span class="math inline">\(N\)</span> 为轨迹的数量，<span class="math inline">\(T\)</span> 为一条轨迹的长度，假设已知策略 <span class="math inline">\(\pi _ { \theta }\)</span> ，那么就可以计算出策略的梯度 <span class="math inline">\(\nabla _ { \theta } \log \pi _ { \theta } ( a | s )\)</span>。另一方面，根据策略 <span class="math inline">\(\pi _ { \theta }\)</span> ，在仿真环境 <span class="math inline">\(E\)</span> 中生成 <span class="math inline">\(N\)</span> 条轨迹的数据，即可计算出 (8)，根据梯度上升 对参数 <span class="math inline">\(\theta\)</span> 进行一步更新，如公式 (9)</p><p><span class="math display">\[\theta \leftarrow \theta + \alpha \nabla _ { \theta } J ( \theta ) \tag{9}\]</span></p><p>总结下来就是：</p><ul><li>增加带来正激励的概率</li><li>减少带来负激励的概率</li></ul><p><img src="https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190117203920.png" width="60%/"></p><h2 id="策略梯度蒙特卡罗-reinforce-算法">策略梯度蒙特卡罗 REINFORCE 算法</h2><p>根据公式 (7) (8) (9) 可得<strong>蒙特卡罗 REINFORCE 算法</strong>流程</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190117091205.png" alt="公式" title>                </div>                <div class="image-caption">公式</div>            </figure><p>写成伪代码形式</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190117091717.png" alt="伪代码" title>                </div>                <div class="image-caption">伪代码</div>            </figure><h2 id="example-高斯策略梯度算法">Example: 高斯策略梯度算法</h2><p>策略属于概率分布，可以用神经网络来表示这种概率分布，输入状态 <span class="math inline">\(s\)</span> ，神经网络将 <span class="math inline">\(s\)</span> 映射成向量 <span class="math inline">\(μ\)</span>，然后网络输出概率 <span class="math inline">\(p ( a | \mu )\)</span> 和动作采样值 <span class="math inline">\(a \sim p ( a | \mu )\)</span>，令 <span class="math inline">\(r\)</span> 为 log 标准差。</p><p><span class="math display">\[\mathcal { N } \left( \text { mean } = \text { NeuralNet } \left( s ; \left\{ W _ { i } , b _ { i } \right\} _ { i = 1 } ^ { L } \right) , stdev = exp(r) \right)\]</span></p><p>其中 <span class="math inline">\(\mu = [ \text { mean, stdev } ]\)</span></p><p><img src="https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190117204712.png"></p><p>在连续的运动空间中，通常使用<strong>高斯策略</strong>，假设方差为 <span class="math inline">\(\sigma ^ { 2 }\)</span> ，策略是高斯的，输入状态 <span class="math inline">\(s\)</span> 输出动作 <span class="math inline">\(a\)</span> 服从 <span class="math inline">\(a \sim \mathcal { N } \left( \mu ( s ) , \sigma ^ { 2 } \right)\)</span>，那么 log 策略梯度为</p><p><span class="math display">\[\nabla _ { \theta } \log \pi _ { \theta } ( s , a ) = \frac { ( a - \mu ( s ) ) \phi ( s ) } { \sigma ^ { 2 } }\tag{10}\]</span></p><p>在实际使用高斯策略时，用神经网络来表示，即令 <span class="math inline">\(f _ { neural\ network } \left( \mathbf { s } _ { t } \right) = \mu ( s _ t )\)</span>，那么策略 <span class="math inline">\(\pi _ \theta\)</span></p><p><span class="math display">\[\log \pi _ { \theta } \left( \mathbf { a } _ { t } | \mathbf { s } _ { t } \right) = - \frac { 1 } { 2 } \left\| f \left( \mathbf { s } _ { t } \right) - \mathbf { a } _ { t } \right\| _ { \Sigma } ^ { 2 } + const \tag{11}\]</span></p><p>策略的梯度</p><p><span class="math display">\[\nabla _ { \theta } \log \pi _ { \theta } \left( \mathbf { a } _ { t } | \mathbf { s } _ { t } \right) = - \frac { 1 } { 2 } \Sigma ^ { - 1 } \left( f \left( \mathbf { s } _ { t } \right) - \mathbf { a } _ { t } \right) \frac { d f } { d \theta } \tag{12}\]</span></p><p>然后反向传播，更新网络参数</p><p><span class="math display">\[- \frac { 1 } { 2 } \Sigma ^ { - 1 } \left( f \left( \mathbf { s } _ { t } \right) - \mathbf { a } _ { t } \right) \left( \sum _ { t } r \left( \mathbf { s } _ { t } , \mathbf { a } _ { t } \right) \right) \tag{13}\]</span></p><h2 id="参考链接">参考链接</h2><ul><li><a href="https://hadovanhasselt.files.wordpress.com/2016/01/pg1.pdf" target="_blank" rel="noopener">Lecture 8: Policy Gradient</a></li><li><a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" target="_blank" rel="noopener">Policy Gradient Algorithms</a></li><li><a href="http://incompleteideas.net/book/the-book-2nd.html" target="_blank" rel="noopener">Reinforcement Learning: An Introduction</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;强化学习策略梯度算法&quot;&gt;强化学习:策略梯度算法&lt;/h1&gt;
&lt;h2 id=&quot;策略梯度的公式推导&quot;&gt;策略梯度的公式推导&lt;/h2&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbo
      
    
    </summary>
    
    
      <category term="Python" scheme="http://wangshub.github.io/tags/Python/"/>
    
      <category term="Algorithm" scheme="http://wangshub.github.io/tags/Algorithm/"/>
    
  </entry>
  
  <entry>
    <title>MLPGaussianPolicy</title>
    <link href="http://wangshub.github.io/2019/01/14/MLPGaussianPolicy/"/>
    <id>http://wangshub.github.io/2019/01/14/MLPGaussianPolicy/</id>
    <published>2019-01-14T11:42:41.000Z</published>
    <updated>2019-04-04T10:10:23.685Z</updated>
    
    <content type="html"><![CDATA[<h1 id="mlp-gaussian-policy">MLP Gaussian policy</h1><p>Looks like a Gaussian policy whose mean and std are outputs of a neural network.</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/tuibot/ImgBed/master/img/20190114195919.png" alt title>                </div>                <div class="image-caption"></div>            </figure><h2 id="参考文献">参考文献</h2><ul><li><a href="https://arxiv.org/pdf/1502.05477.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1502.05477.pdf</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;mlp-gaussian-policy&quot;&gt;MLP Gaussian policy&lt;/h1&gt;
&lt;p&gt;Looks like a Gaussian policy whose mean and std are outputs of a neural network.&lt;/p
      
    
    </summary>
    
    
      <category term="Python" scheme="http://wangshub.github.io/tags/Python/"/>
    
      <category term="Reinforcement Learning" scheme="http://wangshub.github.io/tags/Reinforcement-Learning/"/>
    
      <category term="Tensorflow" scheme="http://wangshub.github.io/tags/Tensorflow/"/>
    
  </entry>
  
  <entry>
    <title>强化学习随机策略之高斯似然数原理与代码实现</title>
    <link href="http://wangshub.github.io/2019/01/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E9%9A%8F%E6%9C%BA%E7%AD%96%E7%95%A5%E4%B9%8B%E9%AB%98%E6%96%AF%E4%BC%BC%E7%84%B6%E6%95%B0%E5%8E%9F%E7%90%86%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0/"/>
    <id>http://wangshub.github.io/2019/01/12/强化学习随机策略之高斯似然数原理与代码实现/</id>
    <published>2019-01-12T06:47:50.000Z</published>
    <updated>2019-04-04T10:10:23.773Z</updated>
    
    <content type="html"><![CDATA[<h1 id="强化学习随机策略之高斯似然数原理与代码实现">强化学习随机策略之高斯似然数原理与代码实现</h1><h2 id="一原理介绍">一、原理介绍</h2><p>使用随机策略有两个关键点</p><ul><li>从策略当中进行采样，获得动作 <span class="math inline">\(a\)</span> (Action)</li><li>计算特定动作的似然数 <span class="math inline">\(\log \pi _ { \theta } ( a | s )\)</span></li></ul><h3 id="什么是多元高斯分布">什么是多元高斯分布？</h3><p>在多元高斯分布中，当协方差矩阵 <span class="math inline">\(\Sigma\)</span> 只有在对角元素非零，而其余元素为 0 时，成为对角高斯分布。 多元高斯分布（Multivariate Gaussian Distribution）是一元高斯分布的在向量形式上的推广，其中向量 <span class="math inline">\(X = \left[ X _ { 1 } , X _ { 2 } , \ldots , X _ { n } \right] ^ { T }\)</span> 的均值为 <span class="math inline">\(\mu \in \mathbf { R } ^ { n }\)</span> ，协方差矩阵为 <span class="math inline">\(\Sigma \in S ^ { n }\)</span> ，概率密度函数表示为</p><p><span class="math display">\[p ( x ; \mu , \Sigma ) = \frac { 1 } { ( 2 \pi ) ^ { n / 2 } | \Sigma | ^ { 1 / 2 } } \exp \left( - \frac { 1 } { 2 } ( x - \mu ) ^ { T } \Sigma ^ { - 1 } ( x - \mu ) \right)\]</span></p><p>例如二元高斯多元分布可以如图所示</p><p><img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fz3n193kb2j21390k2n6d.jpg" width="40%">.</p><p>对于一对随机变量 <span class="math inline">\(X\)</span> 和 <span class="math inline">\(Y\)</span> ，它们的协方差矩阵写作</p><p><span class="math display">\[\operatorname { Cov } [ X , Y ] = E [ ( X - E [ X ] ) ( Y - E [ Y ] ) ] = E [ X Y ] - E [ X ] E [ Y ]\]</span></p><p>对于多个变量的问题，用协方差矩阵 <span class="math inline">\(\Sigma \in S ^ { n }\)</span> 来表示各个变量之间的相关性，有</p><p><span class="math display">\[\Sigma = E \left[ ( X - \mu ) ( X - \mu ) ^ { T } \right] = E \left[ X X ^ { T } \right] - \mu \mu ^ { T }\]</span></p><h3 id="对角多元高斯分布">对角多元高斯分布</h3><p>特殊地，当 N 个随机变量 <span class="math inline">\(X = \left[ X _ { 1 } , X _ { 2 } , \ldots , X _ { n } \right] ^ { T }\)</span> 为各自独立的高斯随机变量时，协方差矩阵为对角阵，即</p><p><span class="math display">\[\Sigma = \operatorname { diag } \left( \sigma _ { 1 } ^ { 2 } , \sigma _ { 2 } ^ { 2 } , \ldots , \sigma _ { n } ^ { 2 } \right)\]</span></p><h3 id="对角高斯策略-diagonal-gaussian-policies">对角高斯策略 Diagonal Gaussian Policies</h3><p>由于标准差的公式 <span class="math inline">\(\sigma = \sqrt { \frac { 1 } { N } \sum _ { i = 1 } ^ { N } \left( x _ { i } - \mu \right) ^ { 2 } }\)</span> 可知 <span class="math inline">\(\sigma\)</span> 始终大于等于 0 ，对标准差取 <code>log</code> 对数，可以将标准差映射到 <span class="math inline">\(( - \infty , \infty )\)</span>，这样更有利于神经网络的训练。</p><ul><li><p><strong>采样</strong>：假设已知动作(Action) 的均值 <span class="math inline">\(\mu _ { \theta } ( s )\)</span> 和标准差 <span class="math inline">\(\sigma _ { \theta } ( s )\)</span> ，引入服从 <span class="math inline">\(( z \sim \mathcal { N } ( 0 , I ) )\)</span> 分布的噪声 <span class="math inline">\(z\)</span> ，下一步的动作采样表示为 <span class="math display">\[a = \mu _ { \theta } ( s ) + \sigma _ { \theta } ( s ) \odot z\]</span> 其中 <span class="math inline">\(\odot\)</span> 表示两个向量之间的内积。</p></li><li><p><strong>似然数</strong>：当均值为 <span class="math inline">\(\mu = \mu _ { \theta } ( s )\)</span> ，标准差为 <span class="math inline">\(\sigma = \sigma _ { \theta } ( s )\)</span> 的 <span class="math inline">\(k-\)</span>维的动作 <span class="math inline">\(a\)</span> 的似然数表示为 <span class="math display">\[\log \pi _ { \theta } ( a | s ) = - \frac { 1 } { 2 } \left( \sum _ { i = 1 } ^ { k } \left( \frac { \left( a _ { i } - \mu _ { i } \right) ^ { 2 } } { \sigma _ { i } ^ { 2 } } + 2 \log \sigma _ { i } \right) + k \log 2 \pi \right)\]</span></p></li></ul><h2 id="二代码实现">二、代码实现</h2><p><strong>要求</strong> - 输入: 样本 <code>x</code>，对角高斯分布的均值和标准差 - 输出：样本 <code>x</code> 的似然数</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">EPS = <span class="number">1e-8</span></span><br></pre></td></tr></table></figure><p>根据上一节，似然数公式，理解公式后就很容易写出代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># my solution</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">my_gaussian_likelihood</span><span class="params">(x, mu, log_std)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">        x: Tensor with shape [batch, dim]</span></span><br><span class="line"><span class="string">        mu: Tensor with shape [batch, dim]</span></span><br><span class="line"><span class="string">        log_std: Tensor with shape [batch, dim] or [dim]</span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">        Tensor with shape [batch]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment">#######################</span></span><br><span class="line">    <span class="comment">#                     #</span></span><br><span class="line">    <span class="comment">#   YOUR CODE HERE    #</span></span><br><span class="line">    <span class="comment">#                     #</span></span><br><span class="line">    <span class="comment">#######################</span></span><br><span class="line">    std = tf.exp(log_std)</span><br><span class="line">    ans = ((x - mu) / std)**<span class="number">2</span> + <span class="number">2</span> * log_std + np.log(<span class="number">2</span> * np.pi)</span><br><span class="line">    ans = <span class="number">-0.5</span> * ans</span><br><span class="line">    <span class="comment"># https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum</span></span><br><span class="line">    sum_ans = tf.reduce_sum(ans, axis=<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> sum_ans</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># standard solution</span></span><br><span class="line"><span class="comment"># 代码来自 spinup/exercises/problem_set_1_solutions/exercise1_2_soln.py</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ans_gaussian_likelihood</span><span class="params">(x, mu, log_std)</span>:</span></span><br><span class="line">    pre_sum = <span class="number">-0.5</span> * (((x-mu)/(tf.exp(log_std)+EPS))**<span class="number">2</span> + <span class="number">2</span>*log_std + np.log(<span class="number">2</span>*np.pi))</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_sum(pre_sum, axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Run this file to verify your solution.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    sess = tf.Session()</span><br><span class="line"></span><br><span class="line">    dim = <span class="number">10</span></span><br><span class="line">    x = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, dim))</span><br><span class="line">    mu = tf.placeholder(tf.float32, shape=(<span class="literal">None</span>, dim))</span><br><span class="line">    log_std = tf.placeholder(tf.float32, shape=(dim,))</span><br><span class="line"></span><br><span class="line">    your_gaussian_likelihood = my_gaussian_likelihood(x, mu, log_std)</span><br><span class="line">    true_gaussian_likelihood = ans_gaussian_likelihood(x, mu, log_std)</span><br><span class="line"></span><br><span class="line">    batch_size = <span class="number">32</span></span><br><span class="line">    feed_dict = &#123;x: np.random.rand(batch_size, dim),</span><br><span class="line">                 mu: np.random.rand(batch_size, dim),</span><br><span class="line">                 log_std: np.random.rand(dim)&#125;</span><br><span class="line"></span><br><span class="line">    your_result, true_result = sess.run([your_gaussian_likelihood, true_gaussian_likelihood],</span><br><span class="line">                                        feed_dict=feed_dict)</span><br><span class="line"></span><br><span class="line">    correct = np.allclose(your_result, true_result)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">"Your answer is"</span>, correct)</span><br></pre></td></tr></table></figure><pre><code>Your answer is True</code></pre><h2 id="参考链接">参考链接</h2><ul><li><p><a href="https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#stochastic-policies" target="_blank" rel="noopener">https://spinningup.openai.com/en/latest/spinningup/rl_intro.html#stochastic-policies</a></p></li><li><p><a href="https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum" target="_blank" rel="noopener">https://www.tensorflow.org/api_docs/python/tf/math/reduce_sum</a></p></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;强化学习随机策略之高斯似然数原理与代码实现&quot;&gt;强化学习随机策略之高斯似然数原理与代码实现&lt;/h1&gt;
&lt;h2 id=&quot;一原理介绍&quot;&gt;一、原理介绍&lt;/h2&gt;
&lt;p&gt;使用随机策略有两个关键点&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;从策略当中进行采样，获得动作 &lt;span class
      
    
    </summary>
    
    
      <category term="Python" scheme="http://wangshub.github.io/tags/Python/"/>
    
      <category term="Reinforcement Learning" scheme="http://wangshub.github.io/tags/Reinforcement-Learning/"/>
    
  </entry>
  
  <entry>
    <title>每个会做饭的人都是人生赢家</title>
    <link href="http://wangshub.github.io/2019/01/11/%E6%AF%8F%E4%B8%AA%E4%BC%9A%E5%81%9A%E9%A5%AD%E7%9A%84%E4%BA%BA%E9%83%BD%E6%98%AF%E4%BA%BA%E7%94%9F%E8%B5%A2%E5%AE%B6/"/>
    <id>http://wangshub.github.io/2019/01/11/每个会做饭的人都是人生赢家/</id>
    <published>2019-01-11T13:19:55.000Z</published>
    <updated>2019-04-04T10:10:23.774Z</updated>
    
    <content type="html"><![CDATA[<p>摘自 《会做饭的，都是人生赢家》</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">不紧不慢地做一顿好吃的饭菜，</span><br><span class="line">是一场可以避世与收获宁静的禅修；</span><br><span class="line">而爱做饭、会做饭的人，</span><br><span class="line">也可以称作是有信仰的人。</span><br><span class="line">这样的人，生命中会少一些波折。</span><br><span class="line">单身的时候，没那么多顾影自怜，</span><br><span class="line">结婚以后，也能好好说话。</span><br><span class="line">毕竟，谁舍得在一锅香喷喷、热腾腾的萝卜牛腩面前大动干戈？</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;摘自 《会做饭的，都是人生赢家》&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span class=&quot;line&quot;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Alfred-Gitlab-Workflow</title>
    <link href="http://wangshub.github.io/2018/10/12/Alfred-Gitlab-Workflow/"/>
    <id>http://wangshub.github.io/2018/10/12/Alfred-Gitlab-Workflow/</id>
    <published>2018-10-12T00:31:33.000Z</published>
    <updated>2019-04-04T10:10:23.684Z</updated>
    
    <content type="html"><![CDATA[<h1 id="alfred-gitlab-workflowgitlab-快速浏览工具">Alfred Gitlab Workflow：Gitlab 快速浏览工具</h1><ul><li><p><strong>GitLab</strong> 是一个类似于 GitHub 的开源源码托管服务，除了代码托管外，Gitlab 还具备了像 Issue、Merge Requests 等软件协作功能。Gitlab 有社区版和企业版，能够被部署到私有服务器上，经常在开发团队中被使用。</p></li><li><p><strong>Alfred</strong> 是 MacOS 下面相当知名的 App 快速启动工具，最迷人的地方在于通过关键词快速触发一系列操作，可以将大部分重复机械的操作写成 Workflow，极大地提升效率。</p></li></ul><p>在我日常使用 Gitlab 过程中，在网页中点击频率最高的莫过于<strong>搜索、Issue、Merge Requests、Todo</strong> 这几个控件了，它们长这样</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fw5629eqelj20ca013q2s.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><p>但是不会“<strong>偷懒</strong>”的程序员都不是好程序员，有时候连 Chrome 都不愿意自己伸手点开，所以对着 Python Alfred Wokflow 库文档，写了一个小插件</p><h2 id="一安装">一、安装</h2><ul><li>依赖于 <a href="https://www.alfredapp.com/" target="_blank" rel="noopener">Alfred3</a></li><li>下载 <a href="https://github.com/wangshub/Alfred-Gitlab-Tool/releases" target="_blank" rel="noopener">Gitlab workflow release</a></li><li>点击安装</li></ul><h2 id="二配置-url-和-token">二、配置 url 和 Token</h2><ul><li><code>glurl</code>：设置 Gitlab 的 url</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fw4431rk7cj20fl035dg6.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li><code>gltoken</code>：设置 Gitlab 的 Token</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fw443ny2tpj20fl03874g.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><h2 id="三功能">三、功能</h2><ul><li><code>glp</code>：快速浏览和模糊搜索 Gitlab 项目</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fw444d697jj20fn0dhq58.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li><code>gli</code>：快速搜索 Gitlab issue</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fw44g9k7ewj20fn09k761.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li><code>glm</code>：快速打开分配给你的 Merge Requests</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fw445iymvkj20fo05vaav.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li><code>gltodo</code>：在默认浏览器中打开 Todo 页面</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fw44jtco12j20fk03baa6.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><h2 id="四项目地址">四、项目地址</h2><ul><li>MIT @ <a href="https://github.com/wangshub" target="_blank" rel="noopener">神奇的战士</a></li><li><a href="https://github.com/wangshub/Alfred-Gitlab-Tool" target="_blank" rel="noopener">https://github.com/wangshub/Alfred-Gitlab-Tool</a></li></ul><h2 id="最后">最后</h2><p>希望这个工具可以为你剩下一点时间撸猫 ~</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;alfred-gitlab-workflowgitlab-快速浏览工具&quot;&gt;Alfred Gitlab Workflow：Gitlab 快速浏览工具&lt;/h1&gt;
&lt;ul&gt;
&lt;li&gt;&lt;p&gt;&lt;strong&gt;GitLab&lt;/strong&gt; 是一个类似于 GitHub 的开源
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>用Python写Alfred3插件</title>
    <link href="http://wangshub.github.io/2018/09/30/%E7%94%A8Python%E5%86%99Alfred3%E6%8F%92%E4%BB%B6/"/>
    <id>http://wangshub.github.io/2018/09/30/用Python写Alfred3插件/</id>
    <published>2018-09-30T10:18:16.000Z</published>
    <updated>2019-04-04T10:10:23.775Z</updated>
    
    <content type="html"><![CDATA[<h1 id="用-python-写-alfred3-workflow-插件一个最小示例">用 Python 写 Alfred3 workflow 插件，一个最小示例</h1><p>以前一直在使用 Cerebro 作为效率工具(开源免费)，但是 Cerebro 原作者似乎很久没有更新，今天抱着试一试的心态用了一把 Alfred 才发现，<strong>哎哟握草</strong></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fvrqrg2gbpj209q08h40i.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><p>Alfred 优化了快速启动与搜索的功能，比 Cerebro 更加丝滑流畅，还引入了 Workflows 这个强大的扩展功能，用户可以用自己熟悉的语言比如 python、JavaScript 和 Php 等来写 Wokflow。因此赋予了 Alfred 无限的可能性。</p><p>我查阅了一下 <a href="https://www.alfredapp.com/help/workflows/" target="_blank" rel="noopener">Alfred 的文档</a>，发现添加一个 Workflow 插件竟是如此便捷。</p><h2 id="背景">背景</h2><p>自己平时喜欢浏览 HackerNews，所以希望在 Alfred 里面快速浏览今天的热门新闻，搜了一下现成的 Alfred 的 HackerNews 插件，都十分老旧，好像都不能正常工作。照着 Alfred 官方文档，用 Python 简单写了个 <a href="https://github.com/wangshub/Alfred-HackerNews" target="_blank" rel="noopener">Alfred-HackerNews</a>，通过请求 API 获取 HackerNews 新闻。</p><h2 id="使用截图">使用截图</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://raw.githubusercontent.com/wangshub/Alfred-HackerNews/master/pic/hn_screen.gif" alt title>                </div>                <div class="image-caption"></div>            </figure><h2 id="步骤">步骤</h2><h3 id="创建一个新-workflow">创建一个新 Workflow</h3><ul><li>新建步骤推荐阅读: <a href="http://www.deanishe.net/alfred-workflow/tutorial_1.html#creating-a-new-workflow" target="_blank" rel="noopener">creating-a-new-workflow</a></li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fvrr8c47v5j20uk0ia794.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li>设置关键词触发 Python 脚本，如图所示，当输入 <code>hn</code> 时，就可以执行 <code>/usr/bin/python hn.py</code> 这条 shell 指令，<code>hn.py</code> 就是需要编辑的 Python 文件。在这个 Workflow 中由于没有输入参数，所以我设置了 <code>No Argument</code>。</li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fvrrlzz4n1j20lq0hs400.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="使用-alfred-workflow-库">使用 Alfred-Workflow 库</h3><p>流程创建完毕后，就需要对代码进行编辑</p><ul><li><p>找到 Workflow 源码路径: 鼠标右键 <code>Hacker-news</code>，选择 <strong>Show in Finder</strong>，找到源码</p></li><li><p>请求 <a href="https://api.hnpwa.com/v0" target="_blank" rel="noopener">api.hnpwa.com/v0</a> 获取 HackerNews 数据</p><p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_top_news</span><span class="params">()</span>:</span></span><br><span class="line">    base_url = <span class="string">'https://api.hnpwa.com/v0/&#123;name&#125;/&#123;page&#125;.json'</span></span><br><span class="line">    max_pages = <span class="number">15</span></span><br><span class="line">    name = <span class="string">'news'</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> page <span class="keyword">in</span> range(<span class="number">1</span>, max_pages):</span><br><span class="line">        url = base_url.format(name=name, page=page)</span><br><span class="line">        req = web.get(url)</span><br><span class="line">        req.raise_for_status()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> len(req.json()) == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        result = result + req.json()</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></table></figure></p></li><li><p>将数据封装成 <code>xml</code> 格式返回给 Alfred <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span><span class="params">(wf)</span>:</span></span><br><span class="line">    posts = wf.cached_data(<span class="string">'posts'</span>, get_top_news, max_age=<span class="number">60</span>*<span class="number">60</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Loop through the returned posts and add an item for each to</span></span><br><span class="line">    <span class="comment"># the list of results for Alfred</span></span><br><span class="line">    <span class="keyword">for</span> post <span class="keyword">in</span> posts:</span><br><span class="line">        subtitle = <span class="string">"points: &#123;points&#125; | user: &#123;user&#125; | &#123;time_ago&#125; | comments:&#123;comments_count&#125; | &#123;url&#125;"</span>.format(</span><br><span class="line">            points=post[<span class="string">'points'</span>],</span><br><span class="line">            user=post[<span class="string">'user'</span>],</span><br><span class="line">            time_ago=post[<span class="string">'time_ago'</span>],</span><br><span class="line">            comments_count=post[<span class="string">'comments_count'</span>],</span><br><span class="line">            url=post[<span class="string">'url'</span>]</span><br><span class="line">        )</span><br><span class="line">        wf.add_item(title=post[<span class="string">'title'</span>],</span><br><span class="line">                    subtitle=subtitle,</span><br><span class="line">                    arg=post[<span class="string">'url'</span>],</span><br><span class="line">                    valid=<span class="literal">True</span>,</span><br><span class="line">                    icon=<span class="string">'./icon.png'</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Send the results to Alfred as XML</span></span><br><span class="line">    wf.send_feedback()</span><br></pre></td></tr></table></figure></p></li><li><p>为了防止大量请求 API 造成服务器压力，也为了更快地显示结果，将数据进行缓存 <figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">posts = wf.cached_data(<span class="string">'posts'</span>, get_top_news, max_age=<span class="number">60</span>*<span class="number">60</span>)</span><br></pre></td></tr></table></figure></p></li></ul><h3 id="导出-.workflow">导出 *.workflow</h3><p>右键 <code>Hacker-news</code> 将 workflow 导出为 <code>hacker-news.alfredworkflow</code> 格式，就可以发布出去啦</p><h2 id="总结">总结</h2><ul><li>如果使用 Python 第三方库，需要 pip 安装到本地路径</li><li>出于兼容性的考虑，Alfred-Workflow 暂时只支持 Python 2，有些小小遗憾；</li></ul><h2 id="示例代码和-workflow-下载地址">示例代码和 Workflow 下载地址</h2><ul><li><a href="https://github.com/wangshub/Alfred-HackerNews" target="_blank" rel="noopener">https://github.com/wangshub/Alfred-HackerNews</a></li><li>MIT@<a href="https://github.com/wangshub/Alfred-HackerNews" target="_blank" rel="noopener">神奇的战士</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;用-python-写-alfred3-workflow-插件一个最小示例&quot;&gt;用 Python 写 Alfred3 workflow 插件，一个最小示例&lt;/h1&gt;
&lt;p&gt;以前一直在使用 Cerebro 作为效率工具(开源免费)，但是 Cerebro 原作者似乎
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>在roboschool中控制Atlas</title>
    <link href="http://wangshub.github.io/2018/09/20/%E5%9C%A8roboschool%E4%B8%AD%E6%8E%A7%E5%88%B6Atlas/"/>
    <id>http://wangshub.github.io/2018/09/20/在roboschool中控制Atlas/</id>
    <published>2018-09-20T09:28:18.000Z</published>
    <updated>2019-04-04T10:10:23.771Z</updated>
    
    <content type="html"><![CDATA[<h1 id="利用-python-在-openai-的-roboschool-中控制-atlas-机器人">利用 Python 在 OpenAI 的 roboschool 中控制 Atlas 机器人</h1><blockquote><p>作者：神奇的战士 Blog: http://thinkhard.tech/</p></blockquote><h2 id="背景介绍">背景介绍</h2><ul><li><strong>Atlas</strong>: Atlas 机器人是一个双足人形机器人，由鼎鼎大名的美国波士顿动力公司为主开发，身高 1.75 米，体重 82kg，它的出现将人形机器人的发展带到了更高的高度。目前可以完成行走、跑步和翻跟头等一系列复杂的动作。作为机器人当中的贵族，价格自然不菲 T_T (更何况还买不到...)。所以只能通过<strong>仿真软件</strong>在里面过下干瘾。</li><li><strong>Roboschool</strong>: Roboschool 是基于 OpenAI Gym 强化学习仿真包的物理仿真引擎。由于 MuJuCo 不开源且收费，所以 OpenAI 的大佬们将 Roboschool 作为 MuJuCo 的替代品。可以在一个场景当中训练多个 Agent 并且完成一挑战任务。</li></ul><h2 id="软件环境安装">软件环境安装</h2><ul><li><a href="https://github.com/openai/roboschool" target="_blank" rel="noopener">Rooboschool</a></li><li><a href="https://github.com/openai/gym" target="_blank" rel="noopener">Gym</a></li></ul><h2 id="示例代码">示例代码</h2><p>在 2017 年 7 月 17 号，Roboschool 发布了 Version 1.1 版本，其中导入了 Atlat 机器人模型，相关新闻可见 <a href="https://github.com/openai/roboschool#news" target="_blank" rel="noopener">NEWS</a></p><p>所以在安装好环境后，可以从 <a href="https://github.com/openai/roboschool/blob/master/agent_zoo/RoboschoolAtlasForwardWalk_v1_2017jul.py" target="_blank" rel="noopener">RoboschoolAtlasForwardWalk_v1_2017jul.py</a> 中找到控制 Atlas 前进走路的源码。</p><ul><li><a href="https://github.com/openai/roboschool/blob/master/agent_zoo/RoboschoolAtlasForwardWalk_v1_2017jul.py" target="_blank" rel="noopener">RoboschoolAtlasForwardWalk_v1_2017jul.py</a></li></ul><h2 id="仿真">仿真</h2><p>Python 运行代码，由于在 Train 的时候，没有对机器人上肢运动进行约束和优化，只关注了下肢的移动，最终训练的结果有点辣眼睛，我们可以看到一段魔性嚣张的步伐</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">python3 RoboschoolAtlasForwardWalk_v1_2017jul.py</span><br></pre></td></tr></table></figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="./picture/atlas_run.gif" alt title>                </div>                <div class="image-caption"></div>            </figure><ul><li>鼠标左键 ： 旋转镜头</li><li>鼠标右键 ： 镜头平移</li><li>按键 <code>F1</code> : 开/关 慢动作</li><li>按键 <code>F2</code> : 隐藏/显示 仿真步数和累积 Rewards</li><li>按键 <code>F3</code> : 隐藏/显示 Action 和 Observation</li></ul><p>在 <a href="https://zhuanlan.zhihu.com/p/40673328" target="_blank" rel="noopener">机器人强化学习之使用 OpenAI Gym 教程与笔记</a> 中介绍过使用导入 Gym 模型和查看 Observation 和 Action 的相关参数，对于 Atlas 也同样适用：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> roboschool</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">"RoboschoolAtlasForwardWalk-v1"</span>)</span><br><span class="line">print(<span class="string">'action_space = '</span>, env.action_space)</span><br><span class="line">print(<span class="string">'observation_space = '</span>, env.observation_space)</span><br></pre></td></tr></table></figure><p>输出</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">action_space = Box(30,)</span><br><span class="line">observation_space = Box(70,)</span><br></pre></td></tr></table></figure><p>从 <code>env.action_space</code> 和 <code>env.observation_space</code> 可知 Action 维度为 30，Observation 维度为 70。</p><ul><li>Action 可以理解为机器人有 30 个自由度，可以通过 PD 控制器控制关节角度；</li><li>Observation 为当前环境的观测向量；</li></ul><p>至于Atlas 机器人 Action 和 Observation 详细解释，在 Roboschool 的 Wiki 中没有相关说明，OpenAI 的开发者建议用户靠<strong>猜</strong>的方式来确定 :)。 确定方式为</p><ol type="1"><li>修改 Roboschool 源码，将 <a href="https://github.com/openai/roboschool/blob/master/roboschool/gym_atlas.py#L21" target="_blank" rel="noopener">gym_atlas.py</a> 重力修改为 0</li><li>按照排除法，控制变量 Action，然后 <code>env.step(action)</code> ，分别记下向量每个元素对应的自由度关节。</li></ol><h2 id="代码说明">代码说明</h2><p>Roboschool 开发者解释到由于强化学习训练 Atlas 走路的代码太杂乱了，所以暂时没有 Train 的代码，所以求人不如求己，后面得自己撸代码 ~ 总体思路是获得 weight 矩阵，即最佳 Policy，输入当前观测向量 Observation 获得下一步的 Action 向量来控制 Atlas 行走。</p><h2 id="总结">总结</h2><ul><li>Roboschool 这个项目适合验证一些简单的强化学习任务，对于新引入的模型支持得还不太友好。</li><li>基本上没有什么成体系的说明文档，所以需要靠多阅读 Roboschool 和 Gym 的 Python 源码来使用。</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;利用-python-在-openai-的-roboschool-中控制-atlas-机器人&quot;&gt;利用 Python 在 OpenAI 的 roboschool 中控制 Atlas 机器人&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;作者：神奇的战士 Blog: htt
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>将 Kindle 转换成文艺电子时钟</title>
    <link href="http://wangshub.github.io/2018/08/05/turn-kindle-to-electron-clock/"/>
    <id>http://wangshub.github.io/2018/08/05/turn-kindle-to-electron-clock/</id>
    <published>2018-08-05T02:30:01.000Z</published>
    <updated>2019-04-04T10:10:23.769Z</updated>
    
    <content type="html"><![CDATA[<h1 id="破解-kindle改造成文艺电子时钟">破解 Kindle，改造成文艺电子时钟</h1><h2 id="前言">前言</h2><h2 id="原材料准备">原材料准备</h2><h3 id="硬件">硬件</h3><ul><li>亚马逊 Kindle 6 inch</li><li>Micro USB 线</li></ul><h3 id="软件">软件</h3><ul><li>SSH 工具</li></ul><h2 id="破解-kindle">破解 Kindle</h2><p>为了让 Kindle 显示我们想要的内容，需要能够登录进 Kindle 系统并且修改系统文件与配置。所以需要将 Kindle 进行<code>越狱</code>操作。</p><ul><li><p>越狱说明请见 <a href="https://wiki.mobileread.com/wiki/Kindle_Screen_Saver_Hack_for_all_2.x,_3.x_%26_4.x_Kindles" target="_blank" rel="noopener">Kindle Screen Saver Hack</a></p></li><li><p>Kindle 上的软件安装包请见 <a href="https://www.mobileread.com/forums/showthread.php?t=88004" target="_blank" rel="noopener">software custom</a></p></li></ul><h2 id="参考链接">参考链接</h2><ul><li><a href="https://www.instructables.com/id/Literary-Clock-Made-From-E-reader/" target="_blank" rel="noopener">https://www.instructables.com/id/Literary-Clock-Made-From-E-reader/</a></li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;破解-kindle改造成文艺电子时钟&quot;&gt;破解 Kindle，改造成文艺电子时钟&lt;/h1&gt;
&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;
&lt;h2 id=&quot;原材料准备&quot;&gt;原材料准备&lt;/h2&gt;
&lt;h3 id=&quot;硬件&quot;&gt;硬件&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;亚马逊 Kindle 6
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器人强化学习之使用 OpenAI Gym 教程与笔记</title>
    <link href="http://wangshub.github.io/2018/07/27/%E6%9C%BA%E5%99%A8%E4%BA%BA%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8B%E4%BD%BF%E7%94%A8%20OpenAI%20Gym%20%E6%95%99%E7%A8%8B%E4%B8%8E%E7%AC%94%E8%AE%B0/"/>
    <id>http://wangshub.github.io/2018/07/27/机器人强化学习之使用 OpenAI Gym 教程与笔记/</id>
    <published>2018-07-27T02:34:40.000Z</published>
    <updated>2019-04-04T10:10:23.773Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器人强化学习之使用-openai-gym-教程与笔记">机器人强化学习之使用 OpenAI Gym 教程与笔记</h1><blockquote><p>除了试图直接去建立一个可以模拟成人大脑的程序之外， 为什么不试图建立一个可以模拟小孩大脑的程序呢?如果它接 受适当的教育，就会获得成人的大脑。 — 阿兰·图灵</p></blockquote><h2 id="介绍">介绍</h2><p>强化学习 (Reinforcement learning) 是机器学习的一个子领域用于制定决策和运动自由度控制。强化学习主要研究在复杂未知的环境中，智体(agent)实现某个目标。强化学习最引人入胜的两个特点是</p><ul><li><p><strong>强化学习非常通用，可以用来解决需要作出一些列决策的所有问题：</strong>例如，训练机器人跑步和弹跳，制定商品价格和库存管理，玩 Atari 游戏和棋盘游戏等等。</p></li><li><p><strong>强化学习已经可以在许多复杂的环境中取得较好的实验结果：</strong>例如 Deep RL 的 Alpha Go等</p></li></ul><p><a href="https://gym.openai.com/docs/" target="_blank" rel="noopener">Gym</a> 是一个研究和开发强化学习相关算法的仿真平台。</p><ul><li>无需智体先验知识；</li><li>兼容常见的数值运算库如 TensorFlow、Theano 等</li></ul><h2 id="gym-的一个最小例子-cartpole-v0">Gym 的一个最小例子 <code>CartPole-v0</code></h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>)</span><br><span class="line">env.reset()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    env.render()</span><br><span class="line">    env.step(env.action_space.sample()) <span class="comment"># take a random action</span></span><br></pre></td></tr></table></figure><p><strong>运行效果</strong></p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1ftmzl7ss9aj20a804a0sj.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><p>至此，第一个 Hello world 就算正式地跑起来了！</p><h2 id="观测observations">观测(Observations)</h2><p>在第一个小栗子中，使用了 <code>env.step()</code> 函数来对每一步进行仿真，在 Gym 中，<code>env.step()</code> 会返回 4 个参数：</p><ul><li><p><strong>观测</strong> Observation (Object)：当前 step 执行后，环境的观测(类型为对象)。例如，从相机获取的像素点，机器人各个关节的角度或棋盘游戏当前的状态等；</p></li><li><p><strong>奖励</strong> Reward (Float): 执行上一步动作(action)后，智体(agent)获得的奖励(浮点类型)，不同的环境中奖励值变化范围也不相同，但是强化学习的目标就是使得总奖励值最大；</p></li><li><p><strong>完成</strong> Done (Boolen): 表示是否需要将环境重置 <code>env.reset</code>。大多数情况下，当 <code>Done</code> 为 <code>True</code> 时，就表明当前回合(episode)或者试验(tial)结束。例如当机器人摔倒或者掉出台面，就应当终止当前回合进行重置(reset);</p></li><li><p><strong>信息</strong> Info (Dict): 针对调试过程的诊断信息。在标准的智体仿真评估当中不会使用到这个 info，具体用到的时候再说。</p></li></ul><p>总结来说，这就是一个强化学习的基本流程，在每个时间点上，智体执行 action，环境返回上一次 action 的观测和奖励，用图表示为</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1ftn0jm54q0j20gm082t8y.jpg" alt="智体与环境交互" title>                </div>                <div class="image-caption">智体与环境交互</div>            </figure><p>在 Gym 仿真中，每一次回合开始，需要先执行 <code>reset()</code> 函数，返回初始观测信息，然后根据标志位 <code>done</code> 的状态，来决定是否进行下一次回合。代码表示为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>)</span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(<span class="number">20</span>):</span><br><span class="line">    observation = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">        env.render()</span><br><span class="line">        print(observation)</span><br><span class="line">        action = env.action_space.sample()</span><br><span class="line">        observation, reward, done, info = env.step(action)</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            print(<span class="string">"Episode finished after &#123;&#125; timesteps"</span>.format(t+<span class="number">1</span>))</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p>仿真截图如下</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fto3j0pf0yj20go0bqdfu.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><p>每次 <code>action</code> 前，将上一次 <code>observation</code> 打印，可以得到打印日志如下</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">[ 0.0349103   1.14771978 -0.03934506 -1.64631971]</span><br><span class="line">[ 0.0578647   1.34327926 -0.07227145 -1.95099638]</span><br><span class="line">[ 0.08473028  1.14899616 -0.11129138 -1.68156178]</span><br><span class="line">[ 0.1077102   0.95532555 -0.14492261 -1.42550525]</span><br><span class="line">[ 0.12681672  1.15191062 -0.17343272 -1.75974995]</span><br><span class="line">[ 0.14985493  0.95912509 -0.20862772 -1.52564382]</span><br><span class="line">Episode finished after 16 timesteps</span><br><span class="line">[ 0.03628829 -0.03189712 -0.01997778  0.02529094]</span><br><span class="line">[ 0.03565035 -0.22672696 -0.01947196  0.31160431]</span><br><span class="line">[ 0.03111581 -0.42156616 -0.01323988  0.59808332]</span><br><span class="line">[ 0.02268449 -0.61650037 -0.00127821  0.8865666 ]</span><br></pre></td></tr></table></figure><h2 id="空间spaces">空间（Spaces）</h2><p>在前面的两个小栗子中，每次执行的动作(action)都是从环境动作空间中随机进行选取的，但是这些动作 (action) 是什么?在 Gym 的仿真环境中，有运动空间 <code>action_space</code> 和观测空间 <code>observation_space</code> 两个指标，程序中被定义为 <code>Space</code> 类型，用于描述有效的运动和观测的格式和范围。下面是一个代码示例</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>)</span><br><span class="line">print(env.action_space)</span><br><span class="line"><span class="comment">#&gt; Discrete(2)</span></span><br><span class="line">print(env.observation_space)</span><br><span class="line"><span class="comment">#&gt; Box(4,)</span></span><br></pre></td></tr></table></figure><pre><code>[33mWARN: gym.spaces.Box autodetected dtype as &lt;class &#39;numpy.float32&#39;&gt;. Please provide explicit dtype.[0mDiscrete(2)Box(4,)</code></pre><p>从程序运行结果来看</p><ul><li><p><code>action_space</code> 是一个离散 <code>Discrete</code> 类型，从 <a href="https://github.com/openai/gym/blob/master/gym/spaces/discrete.py" target="_blank" rel="noopener">discrete.py</a> 源码可知，范围是一个 <code>{0,1,...,n-1}</code> 长度为 <code>n</code> 的非负整数集合，在 <code>CartPole-v0</code> 例子中，动作空间表示为 <code>{0,1}</code>。</p></li><li><p><code>observation_space</code> 是一个 <code>Box</code> 类型，从 <a href="https://github.com/openai/gym/blob/master/gym/spaces/box.py" target="_blank" rel="noopener">box.py</a> 源码可知，表示一个 <code>n</code> 维的盒子，所以在上一节打印出来的 <code>observation</code> 是一个长度为 4 的数组。数组中的每个元素都具有上下界。</p></li></ul><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(env.observation_space.high)</span><br><span class="line">print(env.observation_space.low)</span><br></pre></td></tr></table></figure><pre><code>[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38][-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]</code></pre><p>利用运动空间和观测空间的定义和范围，可以将代码写得更加通用。在许多仿真环境中，<code>Box</code> 和 <code>Discrete</code> 是最常见的空间描述，在智体每次执行动作时，都属于这些空间范围内，代码示例为</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gym <span class="keyword">import</span> spaces</span><br><span class="line">space = spaces.Discrete(<span class="number">8</span>) <span class="comment"># Set with 8 elements &#123;0, 1, 2, ..., 7&#125;</span></span><br><span class="line">x = space.sample()</span><br><span class="line">print(space.contains(x)) </span><br><span class="line">print(space.n == <span class="number">8</span>)</span><br></pre></td></tr></table></figure><pre><code>TrueTrue</code></pre><p>在 <code>CartPole-v0</code> 栗子中，运动只能选择左和右，分别用 <code>{0,1}</code> 表示</p><h2 id="gym-中可用的环境">Gym 中可用的环境</h2><p>Gym 中从简单到复杂，包含了许多经典的<a href="https://gym.openai.com/envs/#classic_control" target="_blank" rel="noopener">仿真环境</a>和各种数据，其中包括</p><ul><li><p>经典控制和文字游戏：经典的强化学习示例，方便入门；</p></li><li><p>算法：从例子中学习强化学习的相关算法，在 Gym 的仿真算法中，由易到难方便新手入坑；</p></li><li><p>雅达利游戏：利用强化学习来玩雅达利的游戏。Gym 中集成了对强化学习有着重要影响的 <a href="http://www.arcadelearningenvironment.org/" target="_blank" rel="noopener">Arcade Learning Environment</a>，并且方便用户安装；</p></li><li><p>2D 和 3D 的机器人：这个是我一直很感兴趣的一部分，在 Gym 中控制机器人进行仿真。需要利用第三方的物理引擎如 <code>MuJoCo</code> 。</p></li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fto4z7mno0j20oa0ikqbs.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><h2 id="注册表">注册表</h2><p>Gym 是一个包含各种各样强化学习仿真环境的大集合，并且封装成通用的接口暴露给用户，查看所有环境的代码如下：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> gym <span class="keyword">import</span> envs</span><br><span class="line">print(envs.registry.all())</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dict_values([EnvSpec(Copy-v0), EnvSpec(RepeatCopy-v0), EnvSpec(ReversedAddition-v0), EnvSpec(ReversedAddition3-v0), EnvSpec(DuplicatedInput-v0), EnvSpec(Reverse-v0), EnvSpec(CartPole-v0), EnvSpec(CartPole-v1), EnvSpec(MountainCar-v0), EnvSpec(MountainCarContinuous-v0), EnvSpec(Pendulum-v0), EnvSpec(Acrobot-v1), EnvSpec(LunarLander-v2), EnvSpec(LunarLanderContinuous-v2), EnvSpec(BipedalWalker-v2),...</span><br></pre></td></tr></table></figure><p>Gym 支持将用户制作的环境写入到注册表中，需要执行 <code>gym.make()</code> 和在启动时注册 <code>register</code>，例如</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">register(</span><br><span class="line">    id=<span class="string">'CartPole-v0'</span>,</span><br><span class="line">    entry_point=<span class="string">'gym.envs.classic_control:CartPoleEnv'</span>,</span><br><span class="line">    max_episode_steps=<span class="number">200</span>,</span><br><span class="line">    reward_threshold=<span class="number">195.0</span>,</span><br><span class="line">)</span><br></pre></td></tr></table></figure><h2 id="参考链接">参考链接</h2><ul><li><p><a href="https://gym.openai.com/docs/" target="_blank" rel="noopener">https://gym.openai.com/docs/</a></p></li><li><p><a href="https://nndl.github.io/" target="_blank" rel="noopener">https://nndl.github.io/</a></p></li></ul><h2 id="结语">结语</h2><p>emmmm ... 第一篇强化学习入坑笔记写完，大多是从官方文档看过来的加上了一点点自己的理解，建议文档这东西还是直接看官方的吧，原汁原味</p><h2 id="关于作者">关于作者</h2><ul><li>神奇的战士</li><li>博客：<a href="http://thinkhard.tech/" target="_blank" rel="noopener">http://thinkhard.tech/</a></li><li>Github: <a href="https://github.com/wangshub" target="_blank" rel="noopener">https://github.com/wangshub</a></li><li>微信公众号：<strong>神奇的战士</strong></li></ul><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fs09ydtc98j20vd06p759.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器人强化学习之使用-openai-gym-教程与笔记&quot;&gt;机器人强化学习之使用 OpenAI Gym 教程与笔记&lt;/h1&gt;
&lt;blockquote&gt;
&lt;p&gt;除了试图直接去建立一个可以模拟成人大脑的程序之外， 为什么不试图建立一个可以模拟小孩大脑的程序呢?如果它接 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>机器人强化学习笔记（0）</title>
    <link href="http://wangshub.github.io/2018/07/03/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
    <id>http://wangshub.github.io/2018/07/03/强化学习笔记/</id>
    <published>2018-07-03T14:34:36.000Z</published>
    <updated>2019-04-04T10:10:23.772Z</updated>
    
    <content type="html"><![CDATA[<h1 id="机器人强化学习笔记0">机器人强化学习笔记（0）</h1><h2 id="机器学习分类">机器学习分类</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fsx1t71czwj20lc0dkdjg.jpg" alt="Machine Learning" title>                </div>                <div class="image-caption">Machine Learning</div>            </figure><h2 id="强化学习问题">强化学习问题</h2><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fsx1s6yxgsj20c204vjrh.jpg" alt="RL problem" title>                </div>                <div class="image-caption">RL problem</div>            </figure><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fsx1xqsh7ij20c204v0t0.jpg" alt="Child walk" title>                </div>                <div class="image-caption">Child walk</div>            </figure><ul><li><strong>监督学习</strong>：监督学习由任务驱动，需要创造外部的“导师”，“导师”拥有外部环境的所有先验信息，并教导 Agent 完成特定的任务。但是 Agent 可以用很多种子任务相结合的方式去完成相同的任务。所以创造一个全能的“导师”来训练 Agent 在实际中几乎是不可能的。</li><li><strong>非监督学习</strong>：非监督学习是由数据驱动，主要目的是找到底层的模式而不是映射关系。例如给用户推荐新闻时，非监督学习主要是根据用户先前阅读过的新闻来推荐相似的新闻。</li><li><strong>强化学习</strong>：和上面两种方法相比较，强化学习主要是从自身的经验来获取知识，在输入和输出之间存在着映射关系。强化学习将奖励函数作为行为的反馈。</li></ul><h2 id="解决强化学习问题的框架和算法">解决强化学习问题的框架和算法</h2><p>强化学习需要平衡 <strong>exploration vs exploitation</strong> 困境。</p><h3 id="马尔可夫决策过程markov-decision-process">马尔可夫决策过程(Markov Decision Process)</h3><p>在强化学习场景下，数学模型为马尔可夫决策过程，表示为</p><ul><li>状态集合：S</li><li>动作集合：A</li><li>奖励函数：R</li><li>策略：Pi</li><li>值：V</li></ul><p>从起始状态到结束状态<span class="math inline">\(S\)</span>需要经过动作集合 A。执行每个动作后，都会获得奖励 R，每个动作可能会导致好的或者坏的奖励函数值。策略(Policy)就是采取某个系列动作的方法，并且会相应的得到奖励函数的值。因此，求解强化学习的目标就是要选取最佳策略(Policy)，在所有可能的状态和时间范围内使得评估函数最大，即</p><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1ft298te49zj203001c0si.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><h3 id="最短路径问题shortest-path-problem">最短路径问题(Shortest Path Problem)</h3><figure class="image-bubble">                <div class="img-lightbox">                    <div class="overlay"></div>                    <img src="https://ws1.sinaimg.cn/large/c3a916a7gy1fsx3dp18t2j20md0bq74c.jpg" alt title>                </div>                <div class="image-caption"></div>            </figure><p>求解以最小代价，从地点 A 到地点 F 的最短路径问题，转化为</p><ul><li>节点集合 {A, B, C, D, E, F}</li><li>从点到点移动成为动作，{A-&gt;B, C-&gt;D}</li><li>奖励函数为每条边的花费</li><li>完成 A 点到 F 行走路线成为策略，如 {A-&gt;B-&gt;D-&gt;F}</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;机器人强化学习笔记0&quot;&gt;机器人强化学习笔记（0）&lt;/h1&gt;
&lt;h2 id=&quot;机器学习分类&quot;&gt;机器学习分类&lt;/h2&gt;
&lt;figure class=&quot;image-bubble&quot;&gt;
                &lt;div class=&quot;img-lightbox&quot;&gt;
 
      
    
    </summary>
    
    
      <category term="Python" scheme="http://wangshub.github.io/tags/Python/"/>
    
      <category term="Reinforment Learning" scheme="http://wangshub.github.io/tags/Reinforment-Learning/"/>
    
  </entry>
  
</feed>
